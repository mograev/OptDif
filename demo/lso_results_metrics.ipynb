{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdfc132",
   "metadata": {},
   "source": [
    "# Latent Space Optimization Results Metrics\n",
    "\n",
    "Compute and compare metrics for latent space optimization results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a61126",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57975d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads all img images that live under the provided root directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, version, subdir=\"img_opt\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            version (str): Parent directory that contains data/samples.\n",
    "            subdir (str): Subdirectory under data/samples to look for images.\n",
    "            transform (Optional[Any]): Optional torchvision/Albumentations transform applied to the PIL image.\n",
    "        \"\"\"\n",
    "        self.root = Path(f\"../results/{version}\").expanduser().resolve()\n",
    "        self.files = sorted(\n",
    "            self.root.glob(f\"data/samples/iter_*/{subdir}/*.png\")\n",
    "        )\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f9619",
   "metadata": {},
   "source": [
    "## Fréchet Inception Distance (FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.metrics.fid import FIDScore\n",
    "\n",
    "def get_fid_score(version):\n",
    "    \"\"\"\n",
    "    Compute the Fréchet Inception Distance (FID) score for a given version and iterations.\n",
    "    Args:\n",
    "        version (str): the version identifier for the model.\n",
    "    Returns:\n",
    "        int: FID score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load hparams yaml\n",
    "    hparams = yaml.safe_load(open(f\"../results/{version}/hparams.yaml\", 'r'))\n",
    "    \n",
    "    # Derive min and max property range (that has not been seen during optimization)\n",
    "    opt_min = int(hparams['max_property_value'])\n",
    "    opt_max = 5\n",
    "\n",
    "    # Derive image size\n",
    "    img_size = 512 if version.startswith(\"ctrloralter\") else 256\n",
    "\n",
    "    # Load optimized images as dataset\n",
    "    img_opt_dataset = ImgDataset(\n",
    "        version=version,\n",
    "        subdir=\"img_opt\",\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Initialize FID instance\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    fid_instance = FIDScore(img_size=img_size, device=device, num_workers=0)\n",
    "    \n",
    "    # Load real statistics\n",
    "    fid_instance.load_real_stats(f\"../data/ffhq/inception_stats/size_{img_size}_smile_{opt_min}_{opt_max}.pt\")\n",
    "\n",
    "    # Compute FID score for the optimized images\n",
    "    fid_score = fid_instance.compute_score_from_data(img_opt_dataset)\n",
    "\n",
    "    return fid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fid_score(version=\"ctrloralter_gbo_23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381e3d2",
   "metadata": {},
   "source": [
    "## Perceptual Quality (LPIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4232a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taming.modules.losses.lpips import LPIPS\n",
    "\n",
    "# Initialize LPIPS instance\n",
    "lpips = LPIPS().eval()\n",
    "\n",
    "def get_lpips_score(version):\n",
    "\t\"\"\"\n",
    "\tCompute the Learned Perceptual Image Patch Similarity (LPIPS) score for a given version.\n",
    "\tArgs:\n",
    "\t\tversion (str): the version identifier for the model.\n",
    "\tReturns:\n",
    "\t\tfloat: LPIPS score.\n",
    "\t\"\"\"\n",
    "\t# Move LPIPS to the appropriate device\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tlpips.to(device)\n",
    "\t\n",
    "\t# Load hparams yaml\n",
    "\thparams = yaml.safe_load(open(f\"../results/{version}/hparams.yaml\", 'r'))\n",
    "\n",
    "\t# Derive image size\n",
    "\timg_size = 512 if version.startswith(\"ctrloralter\") else 256\n",
    "\n",
    "\t# Load optimized images as dataset\n",
    "\timg_opt_dataset = ImgDataset(\n",
    "\t\tversion=version,\n",
    "\t\tsubdir=\"img_opt\",\n",
    "\t\ttransform=transforms.Compose([\n",
    "\t\t\ttransforms.Resize((img_size, img_size)),\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t])\n",
    "\t)\n",
    "\n",
    "\t# Load original images as dataset\n",
    "\timg_orig_dataset = ImgDataset(\n",
    "\t\tversion=version,\n",
    "\t\tsubdir=\"img_orig\",\n",
    "\t\ttransform=transforms.Compose([\n",
    "\t\t\ttransforms.Resize((img_size, img_size)),\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t])\n",
    "\t)\n",
    "\n",
    "\t# Convert datasets to tensors\n",
    "\timg_opt_dataset = torch.stack([img for img in img_opt_dataset], dim=0).to(device)\n",
    "\timg_orig_dataset = torch.stack([img for img in img_orig_dataset], dim=0).to(device)\n",
    "\n",
    "\t# Compute LPIPS score for the optimized images\n",
    "\tlpips_score = lpips(img_opt_dataset, img_orig_dataset).mean().cpu().item()\n",
    "\n",
    "\treturn lpips_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56016bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lpips_score(version=\"ctrloralter_gbo_23\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
