{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdfc132",
   "metadata": {},
   "source": [
    "# Latent Space Optimization Results Metrics\n",
    "\n",
    "Compute and compare metrics for latent space optimization results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a61126",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7aae1f",
   "metadata": {},
   "source": [
    "### Results Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4176cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"../results\").expanduser().resolve()\n",
    "\n",
    "def get_result_dir(version : str, seed : int) -> Path:\n",
    "    \"\"\"\n",
    "    Return the path to the first main.log that matches the seed.\n",
    "    Allowed directory names:\n",
    "      <version>_<seed>\n",
    "      <version>_<seed>_<anything>\n",
    "    \"\"\"\n",
    "    # exact match first (no job-id)\n",
    "    exact = BASE_DIR / f\"{version}_{seed}\"\n",
    "    if exact.is_dir():\n",
    "        return exact\n",
    "\n",
    "    # wildcard for any trailing underscore / job-id\n",
    "    pattern = f\"{version}_{seed}_*/\"\n",
    "    matches = sorted(BASE_DIR.glob(pattern))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No log found for seed {seed} under {BASE_DIR}\")\n",
    "    return matches[-1]  # return the most recent match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7237ea",
   "metadata": {},
   "source": [
    "### Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57975d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads all img images that live under the provided root directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, version, seed, subdir=\"img_opt\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            version (str): Parent directory that contains data/samples.\n",
    "            subdir (str): Subdirectory under data/samples to look for images.\n",
    "            transform (Optional[Any]): Optional torchvision/Albumentations transform applied to the PIL image.\n",
    "        \"\"\"\n",
    "        self.root = get_result_dir(version, seed=seed)\n",
    "        self.files = sorted(\n",
    "            self.root.glob(f\"data/samples/iter_*/{subdir}/*.png\")\n",
    "        )\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f9619",
   "metadata": {},
   "source": [
    "## Fréchet Inception Distance (FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.metrics.fid import FIDScore\n",
    "\n",
    "def get_fid_score(version, seeds=[42, 43, 44]):\n",
    "    \"\"\"\n",
    "    Compute the Fréchet Inception Distance (FID) score for a given version.\n",
    "    Args:\n",
    "        version (str): the version identifier for the model.\n",
    "    Returns:\n",
    "        float: The computed FID score.\n",
    "        float: The standard deviation of the FID score across seeds.\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []\n",
    "    for seed in seeds:\n",
    "        # Get the result directory for the given version and seed\n",
    "        result_dir = get_result_dir(version, seed)\n",
    "        \n",
    "        # Load hparams yaml\n",
    "        hparams = yaml.safe_load(open(result_dir / \"hparams.yaml\", 'r'))\n",
    "\n",
    "        # Derive min and max property range (that has not been seen during optimization)\n",
    "        opt_min = int(hparams['max_property_value'])\n",
    "        opt_max = 5\n",
    "\n",
    "        # Derive image size\n",
    "        img_size = 512 if version.startswith(\"ctrloralter\") else 256\n",
    "\n",
    "        # Load optimized images as dataset\n",
    "        img_opt_dataset = ImgDataset(\n",
    "            version=version,\n",
    "            seed=seed,\n",
    "            subdir=\"img_opt\",\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        # Initialize FID instance\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        fid_instance = FIDScore(img_size=img_size, device=device, num_workers=0)\n",
    "        \n",
    "        # Load real statistics\n",
    "        fid_instance.load_real_stats(f\"../data/ffhq/inception_stats/size_{img_size}_smile_{opt_min}_{opt_max}.pt\")\n",
    "\n",
    "        # Compute FID score for the optimized images\n",
    "        fid_score = fid_instance.compute_score_from_data(img_opt_dataset)\n",
    "\n",
    "        scores.append(float(fid_score))\n",
    "\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fid_score(version=\"ex1_sd35f_dngo_train_trustconstr\", seeds=[42, 43, 44])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381e3d2",
   "metadata": {},
   "source": [
    "## Perceptual Quality (LPIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4232a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from taming.modules.losses.lpips import LPIPS\n",
    "\n",
    "# Initialize LPIPS instance\n",
    "lpips = LPIPS().eval()\n",
    "\n",
    "def get_lpips_score(version, seeds=[42, 43, 44]):\n",
    "\t\"\"\"\n",
    "\tCompute the Learned Perceptual Image Patch Similarity (LPIPS) score for a given version.\n",
    "\tArgs:\n",
    "\t\tversion (str): the version identifier for the model.\n",
    "\tReturns:\n",
    "\t\tfloat: LPIPS score.\n",
    "\t\tfloat: Standard deviation of LPIPS score.\n",
    "\t\"\"\"\n",
    "\t# Move LPIPS to the appropriate device\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tlpips.to(device)\n",
    "\t\n",
    "\tscores = []\n",
    "\tfor seed in seeds:\n",
    "\t\t# Get the result directory for the given version and seed\n",
    "\t\tresult_dir = get_result_dir(version, seed)\n",
    "\n",
    "\t\t# Load hparams yaml\n",
    "\t\thparams = yaml.safe_load(open(result_dir / \"hparams.yaml\", 'r'))\n",
    "\n",
    "\t\t# Derive image size\n",
    "\t\timg_size = 512 if version.startswith(\"ctrloralter\") else 256\n",
    "\n",
    "\t\t# Load optimized images as dataset\n",
    "\t\timg_opt_dataset = ImgDataset(\n",
    "\t\t\tversion=version,\n",
    "\t\t\tseed=seed,\n",
    "\t\t\tsubdir=\"img_opt\",\n",
    "\t\t\ttransform=transforms.Compose([\n",
    "\t\t\t\ttransforms.Resize((img_size, img_size)),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\ttransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\t\t\t])\n",
    "\t\t)\n",
    "\n",
    "\t\t# Load original images as dataset\n",
    "\t\timg_orig_dataset = ImgDataset(\n",
    "\t\t\tversion=version,\n",
    "\t\t\tseed=seed,\n",
    "\t\t\tsubdir=\"img_orig\",\n",
    "\t\t\ttransform=transforms.Compose([\n",
    "\t\t\t\ttransforms.Resize((img_size, img_size)),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\ttransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\t\t\t])\n",
    "\t\t)\n",
    "\n",
    "\t\t# Convert datasets to tensors\n",
    "\t\timg_opt_dataset = torch.stack([img for img in img_opt_dataset], dim=0).to(device)\n",
    "\t\timg_orig_dataset = torch.stack([img for img in img_orig_dataset], dim=0).to(device)\n",
    "\n",
    "\t\t# Compute LPIPS score for the optimized images\n",
    "\t\tlpips_score = lpips(img_opt_dataset, img_orig_dataset).mean().cpu().item()\n",
    "\t\tscores.append(float(lpips_score))\n",
    "\n",
    "\treturn np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56016bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lpips_score(version=\"ex1_sd35f_dngo_train_trustconstr\", seeds=[42, 43, 44])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f7b91",
   "metadata": {},
   "source": [
    "## TopK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fa872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_k(k, version, seeds=[42, 43, 44], iteration_max=500):\n",
    "    \"\"\"\n",
    "    Compute the Top-K smile score (mean ± std over seeds).\n",
    "    Args:\n",
    "        k (int) : K in “Top-K”.\n",
    "        version (str) : Model/version identifier.\n",
    "        seeds (iterable[int]) : Random seeds to aggregate over.\n",
    "        iteration_max (int) : Max evaluations to consider.\n",
    "    Returns:\n",
    "        tuple(float, float): (mean_topk, std_topk) across the given seeds.\n",
    "    \"\"\"\n",
    "    topk_vals = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Load the result file\n",
    "        result_file = get_result_dir(version, seed) / \"results.npz\"\n",
    "        results = np.load(result_file, allow_pickle=True)\n",
    "\n",
    "        # Smile scores from iteration 1 to iteration_max\n",
    "        scores = results[\"opt_point_properties\"][:iteration_max]\n",
    "\n",
    "        # Sort descending and pick the K-th best (account for short runs)\n",
    "        k_idx = min(k, len(scores)) - 1\n",
    "        topk = np.sort(scores)[::-1][k_idx]\n",
    "        topk_vals.append(topk)\n",
    "\n",
    "    mean_topk = float(np.mean(topk_vals))\n",
    "    std_topk  = float(np.std(topk_vals))\n",
    "\n",
    "    return mean_topk, std_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4647e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k(k=10, version=\"ex1_sd35_gbo_train_pca\", seeds=[42, 43, 44], iteration_max=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af2f75",
   "metadata": {},
   "source": [
    "## Mean Smile Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_smile_score(version, seeds=[42, 43, 44], iteration_max=500):\n",
    "    \"\"\"\n",
    "    Compute the smile score mean and std for a given version and seeds.\n",
    "    Args:\n",
    "        version (str): the version identifier for the model.\n",
    "        seeds (list): list of seeds to quantify variability.\n",
    "        iteration_max (int): Maximum number of iterations to consider for the smile score.\n",
    "    Returns:\n",
    "        float: Mean smile score\n",
    "        float: Std smile score\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the results for the specified version and seeds\n",
    "    scores = []\n",
    "    for seed in seeds:\n",
    "        # Load results dictionary\n",
    "        result_file = get_result_dir(version, seed) / \"results.npz\"\n",
    "        results = np.load(result_file, allow_pickle=True)\n",
    "        \n",
    "\t\t# Get smile scores\n",
    "        opt_point_properties = results['opt_point_properties']\n",
    "        opt_point_properties = opt_point_properties[:iteration_max]  # Limit to the first `iteration_max` iterations\n",
    "        scores.append(opt_point_properties.mean(axis=0))\n",
    "\n",
    "    # Compute mean and std\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "\n",
    "    return mean_score, std_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_smile_score(version=\"ex1_sd35f_dngo_train_trustconstr\", seeds=[42, 43, 44], iteration_max=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc312b",
   "metadata": {},
   "source": [
    "## Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ced5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def get_log_time(version, operation, seeds=[42, 43, 44]):\n",
    "\t\"\"\"\n",
    "\tCompute per-run mean time, then the grand mean ± std across runs.\n",
    "\t\"\"\"\n",
    "\tif operation == \"train\":\n",
    "\t\tline_re = re.compile(r\"\\b\\w+\\s+train done in ([\\d.]+)s\")\n",
    "\telif operation == \"opt\":\n",
    "\t\tline_re = re.compile(r\"\\b\\w+\\s+opt done in ([\\d.]+)s\")\n",
    "\telse:\n",
    "\t\traise ValueError(f\"Unknown operation type: {operation}\")\n",
    "\n",
    "\tmean_times = []\n",
    "\tfor seed in seeds:\n",
    "\t\tlog_path = get_result_dir(version, seed) / \"main.log\"\n",
    "\n",
    "\t\twith open(log_path, 'r') as f:\n",
    "\t\t\tlines = f.readlines()\n",
    "\n",
    "\t\ttimes = []\n",
    "\t\tfor line in lines:\n",
    "\t\t\tmatch = line_re.search(line)\n",
    "\t\t\tif match:\n",
    "\t\t\t\ttimes.append(float(match.group(1)))\n",
    "\n",
    "\t\tif not times:\n",
    "\t\t\traise ValueError(f\"No time found in log for seed {seed}\")\n",
    "\n",
    "\t\tmean_times.append(np.mean(times))\n",
    "\n",
    "\t# Compute mean and std across all seeds\n",
    "\tmean_time = np.mean(mean_times)\n",
    "\tstd_time = np.std(mean_times)\n",
    "\n",
    "\treturn mean_time, std_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_log_time(version=\"ex1_sd35f_dngo_train_lbfgsb\", operation=\"opt\", seeds=[42, 43, 44])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
