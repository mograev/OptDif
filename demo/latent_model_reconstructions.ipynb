{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Model Reconstructions\n",
    "\n",
    "This notebook compares different latent models and hyperparameter settings, and the corresponding training results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.latent_models import *\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Configuration\n",
    "MODEL_LIST = [\n",
    "\t# MODEL_TYPE, VERSION\n",
    "\t(\"LatentVAE\", 16),\n",
    "\t(\"LatentAE\", 1),\n",
    "\t(\"LatentLinearAE\", 4),\n",
    "\t(\"LatentVQVAE\", 7),\n",
    "\t(\"LatentVQVAE\", \"8_2\"),\n",
    "\t(\"LatentVQVAE2\", 0),\n",
    "\t(\"LatentVQVAE2\", \"1_2\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Configuration\n",
    "model_type_mapping = {\n",
    "    \"LatentVAE\": \"latent_vae\",\n",
    "    \"LatentVQVAE\": \"latent_vqvae\",\n",
    "    \"LatentVQVAE2\": \"latent_vqvae2\",\n",
    "    \"LatentAE\": \"latent_ae\",\n",
    "    \"LatentLinearAE\": \"latent_linear_ae\"\n",
    "}\n",
    "model_type_sub = [model_type_mapping[MODEL_LIST[i][0]] for i in range(len(MODEL_LIST))]\n",
    "config_path = [f\"../models/{model_type_sub[i]}/version_{MODEL_LIST[i][1]}/hparams.yaml\" for i in range(len(MODEL_LIST))]\n",
    "ckpt_path = [f\"../models/{model_type_sub[i]}/version_{MODEL_LIST[i][1]}/checkpoints/last.ckpt\" for i in range(len(MODEL_LIST))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Latent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model config\n",
    "config = []\n",
    "for i in range(len(MODEL_LIST)):\n",
    "    with open(config_path[i], \"r\") as f:\n",
    "        config.append(yaml.safe_load(f))\n",
    "\n",
    "# Extract model class\n",
    "model_cls_mapping = {\n",
    "    \"LatentVAE\": LatentVAE,\n",
    "    \"LatentVQVAE\": LatentVQVAE,\n",
    "    \"LatentVQVAE2\": LatentVQVAE2,\n",
    "    \"LatentAE\": LatentAE,\n",
    "    \"LatentLinearAE\": LatentLinearAE\n",
    "}\n",
    "model_cls = [model_cls_mapping[MODEL_LIST[i][0]] for i in range(len(MODEL_LIST))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "model = []\n",
    "for i in range(len(MODEL_LIST)):\n",
    "    curr_model = model_cls[i](\n",
    "        ddconfig=config[i][\"ddconfig\"],\n",
    "        lossconfig=config[i][\"lossconfig\"],\n",
    "        ckpt_path=ckpt_path[i],\n",
    "        ignore_keys=['loss'],\n",
    "    )\n",
    "    curr_model.eval()\n",
    "    model.append(curr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stable Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion VAE model\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "sd_vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-3.5-medium\", subfolder=\"vae\")\n",
    "sd_vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Decoding Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "NUM_SAMPLES = 5\n",
    "\n",
    "# Load (subset of) eval batch\n",
    "eval_batch = torch.load(\"../data/ffhq/eval/batch_256.pt\")\n",
    "eval_batch = eval_batch[:NUM_SAMPLES]\n",
    "\n",
    "# Initialize lists to store images and reconstructions\n",
    "input_img = []\n",
    "recon_img = [\n",
    "    [] for _ in range(len(MODEL_LIST))\n",
    "]\n",
    "\n",
    "# Loop through the selected images\n",
    "for i in range(NUM_SAMPLES):\n",
    "    # Add batch dimension: Change from (C, H, W) to (B, C, H, W)\n",
    "    img_tensor = eval_batch[i].unsqueeze(0)\n",
    "    # Encode the image using the Stable Diffusion VAE\n",
    "    sd_latent = sd_vae.encode(img_tensor).latent_dist.sample()\n",
    "    for i, model_i in enumerate(model):\n",
    "        # Encode and decode the latent using the model\n",
    "        recon = model_i(sd_latent, return_only_recon=True)\n",
    "        # Decode the latents using the Stable Diffusion VAE\n",
    "        sd_recon = sd_vae.decode(recon).sample\n",
    "        # Clip the reconstructions to the range [-1, 1]\n",
    "        sd_recon = torch.clamp(sd_recon, -1, 1)\n",
    "        # Store the images\n",
    "        recon_img[i].append(sd_recon.squeeze(0).permute(1, 2, 0).detach().cpu().numpy())\n",
    "    input_img.append(img_tensor.squeeze(0).permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = len(MODEL_LIST) + 1\n",
    "num_rows = NUM_SAMPLES\n",
    "\n",
    "# Plot the input and reconstructed images\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(num_cols*5, num_rows*5))\n",
    "for i in range(num_rows):\n",
    "    # Plot the input image\n",
    "    ax[0, 0].set_title(\"Input Image\", fontsize=25)\n",
    "    ax[i, 0].imshow((input_img[i] + 1) / 2)\n",
    "    ax[i, 0].axis('off')\n",
    "\n",
    "    # Plot the reconstructed images\n",
    "    for j in range(len(MODEL_LIST)):\n",
    "        ax[0, j + 1].set_title(f\"{MODEL_LIST[j][0]} (v{MODEL_LIST[j][1]})\", fontsize=25)\n",
    "        ax[i, j + 1].imshow((recon_img[j][i] + 1) / 2)\n",
    "        ax[i, j + 1].axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LatentVAEs\n",
    "\n",
    "```python\n",
    "MODEL_LIST = [\n",
    "    # MODEL_TYPE, VERSION\n",
    "    (\"LatentVAE\", 5),\n",
    "    (\"LatentVAE\", 8),\n",
    "    (\"LatentVAE\", 16),\n",
    "    (\"LatentVAE\", 17),\n",
    "    (\"LatentVAE\", 19),\n",
    "    (\"LatentVAE\", 18),\n",
    "    (\"LatentVAE\", 20),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = len(MODEL_LIST) + 1\n",
    "num_rows = NUM_SAMPLES\n",
    "\n",
    "# Plot the input and reconstructed images\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(num_cols*5, num_rows*5))\n",
    "for i in range(num_rows):\n",
    "    # Plot the input image\n",
    "    ax[i, 0].imshow((input_img[i] + 1) / 2)\n",
    "    ax[i, 0].axis('off')\n",
    "\n",
    "    # Plot the reconstructed images\n",
    "    for j in range(len(MODEL_LIST)):\n",
    "        ax[i, j + 1].imshow((recon_img[j][i] + 1) / 2)\n",
    "        ax[i, j + 1].axis('off')\n",
    "        \n",
    "\t# Set custom titles\n",
    "    ax[0, 0].set_title(\"Original\", fontsize=25)\n",
    "    ax[0, 1].set_title(f\"16k → 128 (VAE Loss)\", fontsize=25) # v5\n",
    "    ax[0, 2].set_title(f\"16k → 128 (VAE+Disc. Loss)\", fontsize=25) # v8\n",
    "    ax[0, 3].set_title(f\"16k → 512\", fontsize=25) # v16\n",
    "    ax[0, 4].set_title(f\"16k → 512 (Alt. Design)\", fontsize=25) # v17\n",
    "    ax[0, 5].set_title(f\"16k → 4k\", fontsize=25) # v19\n",
    "    ax[0, 6].set_title(f\"16k → 8k\", fontsize=25) # v18\n",
    "    ax[0, 7].set_title(f\"16k → 8k ($\\\\ell_1$ Loss)\", fontsize=25) # v20\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"vis/latent_vae_recon_comparison.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LatentVAE, LatentAE, LatentLinearAE, LatentVQVAE, LatentVQVAE2, VQVAE2\n",
    "\n",
    "```python\n",
    "MODEL_LIST = [\n",
    "\t# MODEL_TYPE, VERSION\n",
    "\t(\"LatentVAE\", 16),\n",
    "\t(\"LatentAE\", 1),\n",
    "\t(\"LatentLinearAE\", 4),\n",
    "\t(\"LatentVQVAE\", 7),\n",
    "\t(\"LatentVQVAE\", \"8_2\"),\n",
    "\t(\"LatentVQVAE2\", 0),\n",
    "\t(\"LatentVQVAE2\", \"1_2\"),\n",
    "]\n",
    "```\n",
    "\n",
    "The ```VQVAE2``` model is added directly since it does not work in the Stable Diffusion latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VQVAE2 model\n",
    "from src.models.vqvae2 import VQVAE2\n",
    "\n",
    "ckpt_vqvae   = \"../models/vqvae2/version_0_2/checkpoints/last.ckpt\"\n",
    "config_yaml  = \"../models/vqvae2/version_0_2/hparams.yaml\"\n",
    "\n",
    "vqvae2 = VQVAE2.load_from_checkpoint(\n",
    "    ckpt_vqvae,\n",
    "    hparams_file=config_yaml,\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "vqvae2.eval().requires_grad_(False)\n",
    "\n",
    "# Get the same reconstruction for the VQVAE2 model\n",
    "recon_vqvae2 = []\n",
    "for i in range(NUM_SAMPLES):\n",
    "\t# Add batch dimension: Change from (C, H, W) to (B, C, H, W)\n",
    "\timg_tensor = eval_batch[i].unsqueeze(0)\n",
    "\t# Encode and decode the latent using the VQVAE2 model\n",
    "\trecon = vqvae2(img_tensor, return_only_recon=True)\n",
    "\t# Clip the reconstructions to the range [-1, 1]\n",
    "\trecon = torch.clamp(recon, -1, 1)\n",
    "\t# Store the images\n",
    "\trecon_vqvae2.append(recon.squeeze(0).permute(1, 2, 0).detach().cpu().numpy())\n",
    "\n",
    "# Append the VQVAE2 reconstructions to the recon_img list\n",
    "recon_img.append(recon_vqvae2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = len(MODEL_LIST) + 2\n",
    "num_rows = NUM_SAMPLES\n",
    "\n",
    "# Plot the input and reconstructed images\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(num_cols*5, num_rows*5))\n",
    "for i in range(num_rows):\n",
    "    # Plot the input image\n",
    "    ax[i, 0].imshow((input_img[i] + 1) / 2)\n",
    "    ax[i, 0].axis('off')\n",
    "\n",
    "    # Plot the reconstructed images\n",
    "    for j in range(len(MODEL_LIST)+1):\n",
    "        ax[i, j + 1].imshow((recon_img[j][i] + 1) / 2)\n",
    "        ax[i, j + 1].axis('off')\n",
    "\n",
    "\t# Set custom titles\n",
    "    ax[0, 0].set_title(\"Original\", fontsize=25)\n",
    "    ax[0, 1].set_title(f\"LatentVAE\", fontsize=25)\n",
    "    ax[0, 2].set_title(f\"LatentAE\", fontsize=25)\n",
    "    ax[0, 3].set_title(f\"LatentLinearAE\", fontsize=25)\n",
    "    ax[0, 4].set_title(f\"LatentVQVAE\", fontsize=25) # v7\n",
    "    ax[0, 5].set_title(f\"+ larger codebook\", fontsize=25) # v8\n",
    "    ax[0, 6].set_title(f\"LatentVQVAE2\", fontsize=25) # v0\n",
    "    ax[0, 7].set_title(f\"+ larger latent space\", fontsize=25) # v1\n",
    "    ax[0, 8].set_title(f\"VQVAE2\", fontsize=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"vis/latent_model_recon_comparison.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
