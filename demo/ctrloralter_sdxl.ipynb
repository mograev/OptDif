{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8956fab9",
   "metadata": {},
   "source": [
    "# CTRLorALTer SD-XL\n",
    "\n",
    "This notebook visualizes the approach of optimization in the CTRLorALTer space of SD-XL using a sample batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a2ef9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fe492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from src.ctrloralter.model import SDXL\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1698c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set half-precision for the model\n",
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Generator\n",
    "\n",
    "# Manual seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def get_generator(seed=SEED, device=\"cuda\"):\n",
    "\t\"\"\"Get a torch generator with a fixed seed.\"\"\"\n",
    "\treturn Generator(device=device).manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0fb4ab",
   "metadata": {},
   "source": [
    "### Load Evaluation Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384bc56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.load(\"../data/ffhq/eval/batch_1024.pt\", map_location=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55675412",
   "metadata": {},
   "source": [
    "### Load Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08523d2",
   "metadata": {},
   "source": [
    "#### Full B-LoRA (Style+Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e560f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.openclip import VisionModel\n",
    "from src.ctrloralter.mapper_network import SimpleMapper\n",
    "\n",
    "full_cfg_base = {\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints\",\n",
    "\t\"ignore_check\": False,\n",
    "\t\"lora\": {\n",
    "\t\t\"style\": {\n",
    "\t\t\t\"enable\": \"always\",\n",
    "\t\t\t\"optimize\": False,\n",
    "\t\t\t\"ckpt_path\": \"ctrloralter/checkpoints/sdxl_b-lora_256\",\n",
    "\t\t\t\"cfg\": True,\n",
    "\t\t\t\"transforms\": [],\n",
    "\t\t\t\"config\": {\n",
    "\t\t\t\t\"lora_scale\": 1.0,\n",
    "\t\t\t\t\"rank\": 256,\n",
    "\t\t\t\t\"c_dim\": 1024,\n",
    "\t\t\t\t\"adaption_mode\": \"b-lora\",\n",
    "\t\t\t\t\"lora_cls\": \"SimpleLoraLinear\",\n",
    "\t\t\t},\n",
    "\t\t\t\"encoder\": VisionModel(clip_model=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", local_files_only=False),\n",
    "            \"mapper_network\": SimpleMapper(d_model=1024, c_dim=1024)\n",
    "\t\t}\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ae711",
   "metadata": {},
   "source": [
    "#### B-Lora Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.openclip import VisionModel\n",
    "from src.ctrloralter.mapper_network import SimpleMapper\n",
    "\n",
    "style_cfg_base = {\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints\",\n",
    "\t\"ignore_check\": True,\n",
    "\t\"lora\": {\n",
    "\t\t\"style\": {\n",
    "\t\t\t\"enable\": \"always\",\n",
    "\t\t\t\"optimize\": False,\n",
    "\t\t\t\"ckpt_path\": \"ctrloralter/checkpoints/sdxl_b-lora_256\",\n",
    "\t\t\t\"cfg\": True,\n",
    "\t\t\t\"transforms\": [],\n",
    "\t\t\t\"config\": {\n",
    "\t\t\t\t\"lora_scale\": 1.0,\n",
    "\t\t\t\t\"rank\": 256,\n",
    "\t\t\t\t\"c_dim\": 1024,\n",
    "\t\t\t\t\"adaption_mode\": \"b-lora_style\",\n",
    "\t\t\t\t\"lora_cls\": \"SimpleLoraLinear\",\n",
    "\t\t\t},\n",
    "\t\t\t\"encoder\": VisionModel(clip_model=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", local_files_only=False),\n",
    "            \"mapper_network\": SimpleMapper(d_model=1024, c_dim=1024)\n",
    "\t\t}\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb1b4a",
   "metadata": {},
   "source": [
    "#### B-LoRA Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.openclip import VisionModel\n",
    "from src.ctrloralter.mapper_network import SimpleMapper\n",
    "\n",
    "content_cfg_base = {\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints\",\n",
    "\t\"ignore_check\": True,\n",
    "\t\"lora\": {\n",
    "\t\t\"style\": {\n",
    "\t\t\t\"enable\": \"always\",\n",
    "\t\t\t\"optimize\": False,\n",
    "\t\t\t\"ckpt_path\": \"ctrloralter/checkpoints/sdxl_b-lora_256\",\n",
    "\t\t\t\"cfg\": True,\n",
    "\t\t\t\"transforms\": [],\n",
    "\t\t\t\"config\": {\n",
    "\t\t\t\t\"lora_scale\": 1.0,\n",
    "\t\t\t\t\"rank\": 256,\n",
    "\t\t\t\t\"c_dim\": 1024,\n",
    "\t\t\t\t\"adaption_mode\": \"b-lora_content\",\n",
    "\t\t\t\t\"lora_cls\": \"SimpleLoraLinear\",\n",
    "\t\t\t},\n",
    "\t\t\t\"encoder\": VisionModel(clip_model=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", local_files_only=False),\n",
    "            \"mapper_network\": SimpleMapper(d_model=1024, c_dim=1024)\n",
    "\t\t}\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee47a15",
   "metadata": {},
   "source": [
    "#### Add Adapters to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cff34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from src.ctrloralter.utils import add_lora_from_config\n",
    "\n",
    "def add_adapter(model, cfg, device=\"cuda\", dtype=dtype):\n",
    "            \n",
    "\t# wrap it in a DictConfig\n",
    "\tomega_cfg = OmegaConf.create(cfg, flags={\"allow_objects\": True})\n",
    "\n",
    "\treturn add_lora_from_config(model, omega_cfg, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc9406",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c97a5",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl = SDXL(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionXLPipeline\",\n",
    "\tmodel_name=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "\tlocal_files_only=False,\n",
    "    guidance_scale=10,\n",
    ").cuda().eval()\n",
    "\n",
    "# set correct dtype\n",
    "sdxl = sdxl.to(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c26c58",
   "metadata": {},
   "source": [
    "### Add adapters to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ad719",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_mask = add_adapter(sdxl, full_cfg_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ca101",
   "metadata": {},
   "source": [
    "### Predict phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = sdxl.predict_phi(batch.to(dtype), branch_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a89fe",
   "metadata": {},
   "source": [
    "### Sample Images\n",
    "\n",
    "Sample image from the model using the obtained $\\varphi$ as condition. Note that these $\\varphi$ have not been optimized, but are the direct output of the global mapper of the style adapter. So the sampled images can't be seen as optimized images, but rather as some form reconstruction of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75117d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_images = sdxl.sample_custom(\n",
    "    prompt=\"realistic colorized photograph of a person\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi],\n",
    "    generator=get_generator(),\n",
    "    cfg_mask=cfg_mask, # use classifier-free guidance mask\n",
    "    skip_encode=True, # skip encoding conditioning\n",
    "    skip_mapping=True, # skip mapping conditioning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a106c",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e00ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input images (batch) and sampled images (sampled_images) next to each other\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, batch.shape[0], figsize=(2*batch.shape[0], 4))\n",
    "for i in range(batch.shape[0]):\n",
    "\taxes[0, i].imshow(batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "\taxes[0, i].axis('off')\n",
    "\taxes[1, i].imshow(sampled_images[i])\n",
    "\taxes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062f9a3",
   "metadata": {},
   "source": [
    "## Comparison Conditioning Opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2cff4",
   "metadata": {},
   "source": [
    "#### Style reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl = SDXL(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionXLPipeline\",\n",
    "\tmodel_name=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "\tlocal_files_only=False,\n",
    "    guidance_scale=10,\n",
    ").cuda().eval().to(dtype)\n",
    "\n",
    "# Add only style adapter\n",
    "cfg_mask = add_adapter(sdxl, style_cfg_base)\n",
    "\n",
    "# Predict phi\n",
    "phi = sdxl.predict_phi(batch.to(dtype), branch_idx=0)\n",
    "\n",
    "# Sample style images\n",
    "style = sdxl.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi],\n",
    "    generator=get_generator(),\n",
    "    cfg_mask=cfg_mask,\n",
    "    skip_encode=True,\n",
    "    skip_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09c94c",
   "metadata": {},
   "source": [
    "#### Content Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa691da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl = SDXL(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionXLPipeline\",\n",
    "\tmodel_name=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "\tlocal_files_only=False,\n",
    "    guidance_scale=10,\n",
    ").cuda().eval().to(dtype)\n",
    "\n",
    "# Add only content adapter\n",
    "cfg_mask = add_adapter(sdxl, content_cfg_base)\n",
    "\n",
    "# Predict phi\n",
    "phi = sdxl.predict_phi(batch.to(dtype), branch_idx=0)\n",
    "\n",
    "# Sample content images\n",
    "content = sdxl.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi],\n",
    "    generator=get_generator(),\n",
    "    cfg_mask=cfg_mask,\n",
    "    skip_encode=True,\n",
    "    skip_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd76358",
   "metadata": {},
   "source": [
    "#### Style + Content reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl = SDXL(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionXLPipeline\",\n",
    "\tmodel_name=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "\tlocal_files_only=False,\n",
    "    guidance_scale=10,\n",
    ").cuda().eval().to(dtype)\n",
    "\n",
    "# Add full adapter\n",
    "cfg_mask = add_adapter(sdxl, full_cfg_base)\n",
    "\n",
    "# Predict phi\n",
    "phi = sdxl.predict_phi(batch.to(dtype), branch_idx=0)\n",
    "\n",
    "# Sample full images\n",
    "content_style = sdxl.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi],\n",
    "    generator=get_generator(),\n",
    "    cfg_mask=cfg_mask,\n",
    "    skip_encode=True,\n",
    "    skip_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e156585e",
   "metadata": {},
   "source": [
    "#### Visualize all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0bc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(4, batch.shape[0], figsize=(2*batch.shape[0], 2.2*4), squeeze=False)\n",
    "\n",
    "# Row 0: Original images\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, i].imshow(img)\n",
    "\tax[0, i].axis('off')\n",
    "\n",
    "# Row 1: Style images\n",
    "ax[1, 0].set_title(\"Reconstruction based on Style\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[1, i].imshow(style[i])\n",
    "\tax[1, i].axis('off')\n",
    "\n",
    "# Row 2: Content images\n",
    "ax[2, 0].set_title(\"Reconstruction based on Content\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[2, i].imshow(content[i])\n",
    "\tax[2, i].axis('off')\n",
    "\n",
    "# Row 3: Style + Content images\n",
    "ax[3, 0].set_title(\"Reconstruction based on Style + Content\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[3, i].imshow(content_style[i])\n",
    "\tax[3, i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92fbeb",
   "metadata": {},
   "source": [
    "## LoRA Scale Ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d3920",
   "metadata": {},
   "source": [
    "### Style reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "style_scales = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6]\n",
    "\n",
    "for scale in style_scales:\n",
    "\tprint(f\"Testing lora scale: {scale}\")\n",
    "\n",
    "\t# Load model\n",
    "\tsdxl = SDXL(\n",
    "\t\tpipeline_type=\"diffusers.StableDiffusionXLPipeline\",\n",
    "\t\tmodel_name=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "\t\tlocal_files_only=False,\n",
    "\t\tguidance_scale=10,\n",
    "\t).cuda().eval().to(dtype)\n",
    "\n",
    "\t# Set LoRA scale\n",
    "\tstyle_cfg = copy.deepcopy(style_cfg_base)\n",
    "\tstyle_cfg[\"lora\"][\"style\"][\"config\"][\"lora_scale\"] = scale\n",
    "\tcfg_mask = add_adapter(sdxl, style_cfg)\n",
    "\n",
    "\t# Predict phi\n",
    "\tphi = sdxl.predict_phi(batch.to(dtype), branch_idx=0)\n",
    "\n",
    "\t# Sample style images\n",
    "\tstyle = sdxl.sample_custom(\n",
    "\t\tprompt=\"\",\n",
    "\t\tnum_images_per_prompt=batch.shape[0],\n",
    "\t\tcs=[phi],\n",
    "\t\tgenerator=get_generator(),\n",
    "\t\tcfg_mask=cfg_mask,\n",
    "\t\tskip_encode=True,\n",
    "\t\tskip_mapping=True,\n",
    "\t)\n",
    "\n",
    "\t# Append to results\n",
    "\tresults[scale] = style\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(len(results)+1, batch.shape[0], figsize=(2.2*batch.shape[0], 2.2*(len(results)+1)), squeeze=False)\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for j in range(batch.shape[0]):\n",
    "\timg = (batch[j].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, j].imshow(img)\n",
    "\tax[0, j].axis('off')\n",
    "for i, (scale, images) in enumerate(results.items()):\n",
    "\tax[i+1, 0].set_title(f\"LoRA Scale: {scale:.1f}\", loc=\"left\")\n",
    "\tfor j, img in enumerate(images):\n",
    "\t\tax[i+1, j].imshow(img)\n",
    "\t\tax[i+1, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd358bb",
   "metadata": {},
   "source": [
    "### Content reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "content_scales = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6]\n",
    "\n",
    "for scale in content_scales:\n",
    "\tprint(f\"Testing lora scale: {scale}\")\n",
    "\n",
    "\t# Load model\n",
    "\tsdxl = SDXL(\n",
    "\t\tpipeline_type=\"diffusers.StableDiffusionXLPipeline\",\n",
    "\t\tmodel_name=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "\t\tlocal_files_only=False,\n",
    "\t\tguidance_scale=10,\n",
    "\t).cuda().eval().to(dtype)\n",
    "\n",
    "\t# Set LoRA scale\n",
    "\tcontent_cfg = copy.deepcopy(content_cfg_base)\n",
    "\tcontent_cfg[\"lora\"][\"style\"][\"config\"][\"lora_scale\"] = scale\n",
    "\tcfg_mask = add_adapter(sdxl, content_cfg)\n",
    "\n",
    "\t# Predict phi\n",
    "\tphi = sdxl.predict_phi(batch.to(dtype), branch_idx=0)\n",
    "\n",
    "\t# Sample content images\n",
    "\tcontent = sdxl.sample_custom(\n",
    "\t\tprompt=\"\",\n",
    "\t\tnum_images_per_prompt=batch.shape[0],\n",
    "\t\tcs=[phi],\n",
    "\t\tgenerator=get_generator(),\n",
    "\t\tcfg_mask=cfg_mask,\n",
    "\t\tskip_encode=True,\n",
    "\t\tskip_mapping=True,\n",
    "\t)\n",
    "\n",
    "\t# Append to results\n",
    "\tresults[scale] = content\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(len(results)+1, batch.shape[0], figsize=(2.2*batch.shape[0], 2.2*(len(results)+1)), squeeze=False)\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for j in range(batch.shape[0]):\n",
    "\timg = (batch[j].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, j].imshow(img)\n",
    "\tax[0, j].axis('off')\n",
    "for i, (scale, images) in enumerate(results.items()):\n",
    "\tax[i+1, 0].set_title(f\"LoRA Scale: {scale:.1f}\", loc=\"left\")\n",
    "\tfor j, img in enumerate(images):\n",
    "\t\tax[i+1, j].imshow(img)\n",
    "\t\tax[i+1, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15f557",
   "metadata": {},
   "source": [
    "### Style + Content reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "full_scales = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6]\n",
    "\n",
    "for scale in full_scales:\n",
    "\tprint(f\"Testing lora scale: {scale}\")\n",
    "\n",
    "\t# Load model\n",
    "\tsdxl = SDXL(\n",
    "\t\tpipeline_type=\"diffusers.StableDiffusionXLPipeline\",\n",
    "\t\tmodel_name=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "\t\tlocal_files_only=False,\n",
    "\t\tguidance_scale=10,\n",
    "\t).cuda().eval().to(dtype)\n",
    "\n",
    "\t# Set LoRA scale\n",
    "\tfull_cfg = copy.deepcopy(full_cfg_base)\n",
    "\tfull_cfg[\"lora\"][\"style\"][\"config\"][\"lora_scale\"] = scale\n",
    "\tcfg_mask = add_adapter(sdxl, full_cfg)\n",
    "\n",
    "\t# Predict phi\n",
    "\tphi = sdxl.predict_phi(batch.to(dtype), branch_idx=0)\n",
    "\n",
    "\t# Sample images\n",
    "\tcontent_style = sdxl.sample_custom(\n",
    "\t\tprompt=\"\",\n",
    "\t\tnum_images_per_prompt=batch.shape[0],\n",
    "\t\tcs=[phi],\n",
    "\t\tgenerator=get_generator(),\n",
    "\t\tcfg_mask=cfg_mask,\n",
    "\t\tskip_encode=True,\n",
    "\t\tskip_mapping=True,\n",
    "\t)\n",
    "\n",
    "\t# Append to results\n",
    "\tresults[scale] = content_style\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(len(results)+1, batch.shape[0], figsize=(2.2*batch.shape[0], 2.2*(len(results)+1)), squeeze=False)\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for j in range(batch.shape[0]):\n",
    "\timg = (batch[j].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, j].imshow(img)\n",
    "\tax[0, j].axis('off')\n",
    "for i, (scale, images) in enumerate(results.items()):\n",
    "\tax[i+1, 0].set_title(f\"LoRA Scale: {scale:.1f}\", loc=\"left\")\n",
    "\tfor j, img in enumerate(images):\n",
    "\t\tax[i+1, j].imshow(img)\n",
    "\t\tax[i+1, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc85c2",
   "metadata": {},
   "source": [
    "## Guidance Scale Ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3dad9",
   "metadata": {},
   "source": [
    "### Style reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a77f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "guidance_scales = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for scale in guidance_scales:\n",
    "\tprint(f\"Testing guidance scale: {scale}\")\n",
    "\n",
    "\t# Load model\n",
    "\tsdxl = SDXL(\n",
    "\t\tpipeline_type=\"diffusers.StableDiffusionXLPipeline\",\n",
    "\t\tmodel_name=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "\t\tlocal_files_only=False,\n",
    "\t\tguidance_scale=scale,\n",
    "\t).cuda().eval().to(dtype)\n",
    "\n",
    "\t# Set LoRA scale\n",
    "\tcfg_mask = add_adapter(sdxl, style_cfg_base)\n",
    "\n",
    "\t# Predict phi\n",
    "\tphi = sdxl.predict_phi(batch.to(dtype), branch_idx=0)\n",
    "\n",
    "\t# Sample style images\n",
    "\tstyle = sdxl.sample_custom(\n",
    "\t\tprompt=\"\",\n",
    "\t\tnum_images_per_prompt=batch.shape[0],\n",
    "\t\tcs=[phi],\n",
    "\t\tgenerator=get_generator(),\n",
    "\t\tcfg_mask=cfg_mask,\n",
    "\t\tskip_encode=True,\n",
    "\t\tskip_mapping=True,\n",
    "\t)\n",
    "\n",
    "\t# Append to results\n",
    "\tresults[scale] = style\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(len(results)+1, batch.shape[0], figsize=(2.2*batch.shape[0], 2.2*(len(results)+1)), squeeze=False)\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for j in range(batch.shape[0]):\n",
    "\timg = (batch[j].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, j].imshow(img)\n",
    "\tax[0, j].axis('off')\n",
    "for i, (scale, images) in enumerate(results.items()):\n",
    "\tax[i+1, 0].set_title(f\"Guidance Scale: {scale:.1f}\", loc=\"left\")\n",
    "\tfor j, img in enumerate(images):\n",
    "\t\tax[i+1, j].imshow(img)\n",
    "\t\tax[i+1, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
