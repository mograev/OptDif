{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8956fab9",
   "metadata": {},
   "source": [
    "# CTRLorALTer\n",
    "\n",
    "This notebook visualizes the approach of optimization in the CTRLorALTer latent space using a sample batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a2ef9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0fb4ab",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3aa377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "from src.dataloader.ffhq import FFHQWeightedDataset\n",
    "from src.dataloader.weighting import DataWeighter\n",
    "from argparse import Namespace\n",
    "from torchvision import transforms\n",
    "\n",
    "args = Namespace(\n",
    "    img_dir=\"../data/ffhq/images1024x1024\",\n",
    "    img_tensor_dir=\"../data/ffhq/pt_images\",\n",
    "    attr_path=\"../data/ffhq/ffhq_smile_scores.json\",\n",
    "    max_property_value=5,\n",
    "    min_property_value=0,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    val_split=0,\n",
    "    data_device=\"cuda\",\n",
    "    aug=True,\n",
    "    weight_type=\"uniform\",\n",
    "    rank_weight_k=1e-3,\n",
    "    weight_quantile=None,\n",
    "    dbas_noise=None,\n",
    "    rwr_alpha=None,\n",
    ")\n",
    "transform = transforms.Compose([\n",
    "\ttransforms.Resize((512, 512)),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "datamodule = FFHQWeightedDataset(args, DataWeighter(args), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one batch of data\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "batch = batch.to(\"cuda\")\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c97a5",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ec7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.model import SD15\n",
    "\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55675412",
   "metadata": {},
   "source": [
    "### Load Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed63cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cfg = {\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints\",\n",
    "\t\"lora\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08523d2",
   "metadata": {},
   "source": [
    "#### Style Adapter (Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.openclip import VisionModel\n",
    "from src.ctrloralter.mapper_network import SimpleMapper\n",
    "\n",
    "style_cfg = {\n",
    "\t\"enable\": \"always\",\n",
    "\t\"optimize\": False,\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints/sd15-style-cross-160-h\",\n",
    "\t\"ignore_check\": False,\n",
    "\t\"cfg\": True,\n",
    "\t\"transforms\": [],\n",
    "\t\"config\": {\n",
    "\t\t\"lora_scale\": 1.0,\n",
    "\t\t\"rank\": 160,\n",
    "\t\t\"c_dim\": 1024,\n",
    "\t\t\"adaption_mode\": \"only_cross\",\n",
    "\t\t\"lora_cls\": \"SimpleLoraLinear\",\n",
    "\t\t\"broadcast_tokens\": True,\n",
    "\t},\n",
    "\t\"encoder\": VisionModel(clip_model=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", local_files_only=False),\n",
    "\t\"mapper_network\": SimpleMapper(1024, 1024),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ae711",
   "metadata": {},
   "source": [
    "#### Depth Structure Adapter (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.midas import DepthEstimator\n",
    "from src.ctrloralter.mapper_network import FixedStructureMapper15\n",
    "\n",
    "depth_cfg = {\n",
    "\t\"enable\": \"always\",\n",
    "\t\"optimize\": False,\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints/sd15-depth-128-only-res\",\n",
    "\t\"ignore_check\": False,\n",
    "\t\"cfg\": False,\n",
    "\t\"transforms\": [],\n",
    "\t\"config\": {\n",
    "\t\t\"lora_scale\": 0.35,\n",
    "\t\t\"rank\": 128,\n",
    "\t\t\"c_dim\": 128,\n",
    "\t\t\"adaption_mode\": \"only_res_conv\",\n",
    "\t\t\"lora_cls\": \"NewStructLoRAConv\",\n",
    "\t},\n",
    "\t\"encoder\": DepthEstimator(size=512, local_files_only=False),\n",
    "\t\"mapper_network\": FixedStructureMapper15(c_dim=128),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb1b4a",
   "metadata": {},
   "source": [
    "#### HED Structure Adapter (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.hed import TorchHEDdetector\n",
    "from src.ctrloralter.mapper_network import FixedStructureMapper15\n",
    "\n",
    "hed_cfg = {\n",
    "\t\"enable\": \"always\",\n",
    "\t\"optimize\": False,\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints/sd15-hed-128-only-res\",\n",
    "\t\"ignore_check\": False,\n",
    "\t\"cfg\": False,\n",
    "\t\"transforms\": [],\n",
    "\t\"config\": {\n",
    "\t\t\"lora_scale\": 1.0,\n",
    "\t\t\"rank\": 128,\n",
    "\t\t\"c_dim\": 128,\n",
    "\t\t\"adaption_mode\": \"only_res_conv\",\n",
    "\t\t\"lora_cls\": \"NewStructLoRAConv\",\n",
    "\t},\n",
    "\t\"encoder\": TorchHEDdetector(size=512, local_files_only=False),\n",
    "\t\"mapper_network\": FixedStructureMapper15(c_dim=128),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee47a15",
   "metadata": {},
   "source": [
    "### Add Adapters to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cff34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from src.ctrloralter.utils import add_lora_from_config\n",
    "import copy\n",
    "\n",
    "def add_adapters(model, raw_cfg, style_cfg=None, depth_cfg=None, hed_cfg=None, device=\"cuda\"):\n",
    "\tcfg = copy.deepcopy(raw_cfg)\n",
    "\n",
    "\tif style_cfg is not None:\n",
    "\t\tcfg[\"lora\"][\"style\"] = style_cfg\n",
    "\tif depth_cfg is not None:\n",
    "\t\tcfg[\"lora\"][\"struct\"] = depth_cfg\n",
    "\telif hed_cfg is not None:\n",
    "\t\tcfg[\"lora\"][\"struct\"] = hed_cfg\n",
    "            \n",
    "\t# wrap it in a DictConfig\n",
    "\tcfg = OmegaConf.create(cfg, flags={\"allow_objects\": True})\n",
    "\n",
    "\tadd_lora_from_config(model, cfg, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ad719",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_adapters(sd15, raw_cfg, style_cfg=style_cfg, depth_cfg=depth_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ca101",
   "metadata": {},
   "source": [
    "## Predict phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = sd15.predict_phi(batch.to(\"cuda\"), branch_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a89fe",
   "metadata": {},
   "source": [
    "## Sample Images\n",
    "\n",
    "Sample image from the model using the obtained $\\varphi$ as condition. Note that these $\\varphi$ have not been optimized, but are the direct output of the global mapper of the style adapter. So the sampled images can't be seen as optimized images, but rather as some form reconstruction of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75117d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_images = sd15.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[\n",
    "        phi,    # style conditioning\n",
    "        batch,  # structure conditioning\n",
    "    ],\n",
    "    generator=None,\n",
    "    skip_encode=[0], # skip encoding the first conditioning (style, already in phi)\n",
    "    skip_mapping=[0], # skip mapping the first conditioning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input images (batch) and sampled images (sampled_images) next to each other\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, batch.shape[0], figsize=(2*batch.shape[0], 4))\n",
    "for i in range(batch.shape[0]):\n",
    "\taxes[0, i].imshow(batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "\taxes[0, i].axis('off')\n",
    "\taxes[1, i].imshow(sampled_images[i])\n",
    "\taxes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f04bbd",
   "metadata": {},
   "source": [
    "## Supplements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec0079",
   "metadata": {},
   "source": [
    "### Visualize Depth Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e56461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.midas import DepthEstimator\n",
    "\n",
    "de = DepthEstimator(size=512, local_files_only=False).to(\"cuda\").eval()\n",
    "\n",
    "depths = de(batch)\n",
    "depths = depths.mean(dim=1, keepdim=True)  # Average over the color\n",
    "\n",
    "# Visualize input images (batch) and their corresponding depth maps\n",
    "fig, axes = plt.subplots(2, batch.shape[0], figsize=(2*batch.shape[0], 4))\n",
    "for i in range(batch.shape[0]):\n",
    "\taxes[0, i].imshow(batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "\taxes[0, i].axis('off')\n",
    "\taxes[1, i].imshow(depths[i].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "\taxes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa16d5",
   "metadata": {},
   "source": [
    "### Visualize HED Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1674df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.hed import TorchHEDdetector\n",
    "\n",
    "hed = TorchHEDdetector(size=512, local_files_only=False).to(\"cuda\").eval()\n",
    "\n",
    "edges = hed(batch)\n",
    "edges = edges.mean(dim=1, keepdim=True)  # Average over the color\n",
    "\n",
    "# Visualize input images (batch) and their corresponding depth maps\n",
    "fig, axes = plt.subplots(2, batch.shape[0], figsize=(2*batch.shape[0], 4))\n",
    "for i in range(batch.shape[0]):\n",
    "\taxes[0, i].imshow(batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "\taxes[0, i].axis('off')\n",
    "\taxes[1, i].imshow(edges[i].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "\taxes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062f9a3",
   "metadata": {},
   "source": [
    "## All together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2cff4",
   "metadata": {},
   "source": [
    "#### Direct style reconstructions (without structure adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "# Add only style adapter\n",
    "add_adapters(sd15, raw_cfg, style_cfg=style_cfg)\n",
    "\n",
    "# Predict phi\n",
    "phi = sd15.predict_phi(batch.to(\"cuda\"), branch_idx=0)\n",
    "\n",
    "# Sample style images\n",
    "style = sd15.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi],\n",
    "    generator=None,\n",
    "    skip_encode=True,\n",
    "    skip_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09c94c",
   "metadata": {},
   "source": [
    "#### Style + Depth Structure Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa691da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "depth_model = DepthEstimator(size=512, local_files_only=False).to(\"cuda\").eval()\n",
    "depth_maps = depth_model(batch)\n",
    "depth_maps = depth_maps.mean(dim=1, keepdim=True)  # average over color channels\n",
    "\n",
    "# Add only style adapter\n",
    "add_adapters(sd15, raw_cfg, style_cfg=style_cfg, depth_cfg=depth_cfg)\n",
    "\n",
    "# Predict phi\n",
    "phi = sd15.predict_phi(batch.to(\"cuda\"), branch_idx=0)\n",
    "\n",
    "# Sample images\n",
    "style_depth = sd15.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi, batch],  # style and structure conditioning\n",
    "    generator=None,\n",
    "    skip_encode=[0],\n",
    "    skip_mapping=[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd76358",
   "metadata": {},
   "source": [
    "#### Style + HED Structure Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "hed_model = TorchHEDdetector(size=512, local_files_only=False).to(\"cuda\").eval()\n",
    "hed_maps = hed_model(batch)\n",
    "hed_maps = hed_maps.mean(dim=1, keepdim=True)  # average over color channels\n",
    "\n",
    "# Add only style adapter\n",
    "add_adapters(sd15, raw_cfg, style_cfg=style_cfg, hed_cfg=hed_cfg)\n",
    "\n",
    "# Predict phi\n",
    "phi = sd15.predict_phi(batch.to(\"cuda\"), branch_idx=0)\n",
    "\n",
    "# Sample images\n",
    "style_hed = sd15.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi, batch],  # style and structure conditioning\n",
    "    generator=None,\n",
    "    skip_encode=[0],\n",
    "    skip_mapping=[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e156585e",
   "metadata": {},
   "source": [
    "#### Visualize all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0bc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(6, batch.shape[0], figsize=(2*batch.shape[0], 2.2*6), squeeze=False)\n",
    "\n",
    "# Row 0: Original images\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, i].imshow(img)\n",
    "\tax[0, i].axis('off')\n",
    "\n",
    "# Row 1: Style images\n",
    "ax[1, 0].set_title(\"Reconstruction based on Style\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[1, i].imshow(style[i])\n",
    "\tax[1, i].axis('off')\n",
    "\n",
    "# Row 2: Depth Maps\n",
    "ax[2, 0].set_title(\"Depth Maps\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (depth_maps[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[2, i].imshow(img, cmap='gray')\n",
    "\tax[2, i].axis('off')\n",
    "\n",
    "# Row 3: Style + Depth\n",
    "ax[3, 0].set_title(\"Reconstruction based on Style + Depth\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[3, i].imshow(style_depth[i])\n",
    "\tax[3, i].axis('off')\n",
    "\n",
    "# Row 4: HED Maps\n",
    "ax[4, 0].set_title(\"HED Maps\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (hed_maps[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[4, i].imshow(img, cmap='gray')\n",
    "\tax[4, i].axis('off')\n",
    "\n",
    "# Row 5: Style + HED\n",
    "ax[5, 0].set_title(\"Reconstruction based on Style + HED\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[5, i].imshow(style_hed[i])\n",
    "\tax[5, i].axis('off')\n",
    "\t\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
