{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438f936f",
   "metadata": {},
   "source": [
    "# VQVAE2 Sampling\n",
    "\n",
    "This notebook demonstrates how to sample from a VQVAE2 model that uses an autoregressive prior in the latent spaces. It visualizes some samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0de500",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models.latent_models import LatentVQVAE2\n",
    "from src.models.transformer_prior import HierarchicalTransformerPrior\n",
    "from src.models.pixelsnail_prior import HierarchicalPixelSnailPrior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e06ae0",
   "metadata": {},
   "source": [
    "### Load VQVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b62a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_vqvae   = \"../models/latent_vqvae2/version_1/checkpoints/last.ckpt\"\n",
    "config_yaml  = \"../models/latent_vqvae2/version_1/hparams.yaml\"\n",
    "\n",
    "latent_model = LatentVQVAE2.load_from_checkpoint(\n",
    "    ckpt_vqvae,\n",
    "    hparams_file=config_yaml,\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "latent_model.eval().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c97ede",
   "metadata": {},
   "source": [
    "### Load autoregressive prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5dbd0",
   "metadata": {},
   "source": [
    "#### Tranformer prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = HierarchicalTransformerPrior(\n",
    "    vqvae=latent_model,\n",
    "    d_model=768,\n",
    "    n_layers=12,\n",
    "    n_heads=12,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.0,\n",
    ").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_prior = \"../models/latent_prior/version_7/checkpoints/last.ckpt\"\n",
    "\n",
    "prior = HierarchicalTransformerPrior.load_from_checkpoint(\n",
    "    ckpt_prior,\n",
    "    vqvae=latent_model,\n",
    "    map_location=\"cuda\",\n",
    ").eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840604a",
   "metadata": {},
   "source": [
    "#### PixelSNAIL prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a34d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = HierarchicalPixelSnailPrior(\n",
    "    vqvae=latent_model,\n",
    "    n_chan=128,\n",
    "    n_blocks=8,\n",
    "    n_heads=4,\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    dropout=0.1,\n",
    ").eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_prior = \"../models/latent_prior/version_5/checkpoints/last.ckpt\"\n",
    "\n",
    "prior = HierarchicalPixelSnailPrior.load_from_checkpoint(\n",
    "    ckpt_prior,\n",
    "    vqvae=latent_model,\n",
    "    map_location=\"cuda\",\n",
    ").eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c76893",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sd_latents = prior.sample(\n",
    "        n=4,              # how many\n",
    "        temperature=.5,  # lower = sharper, higher = more varied\n",
    "        top_k=64,         # restrict to top-k logits (optional, None = full softmax)\n",
    "    ).cpu()               # imgs are in [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38785e02",
   "metadata": {},
   "source": [
    "## Decode samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e72280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion VAE model\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "sd_vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-3.5-medium\", subfolder=\"vae\")\n",
    "sd_vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d486666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the latents\n",
    "images = sd_vae.decode(sd_latents, return_dict=False)[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7493db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_images(imgs, nrow=4):\n",
    "    \"\"\"Plot a batch of images.\"\"\"\n",
    "    n = len(imgs)\n",
    "    ncols = min(n, nrow)\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 2, nrows * 2))\n",
    "    for i in range(n):\n",
    "        ax = axes[i // ncols, i % ncols]\n",
    "        ax.imshow((imgs[i].permute(1, 2, 0).cpu().numpy().clip(-1,1) + 1) / 2)  # Convert to [0, 1]\n",
    "        ax.axis('off')\n",
    "    for j in range(i + 1, nrows * ncols):\n",
    "        axes[j // ncols, j % ncols].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_images(images, nrow=2)  # Display the generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738daa9a",
   "metadata": {},
   "source": [
    "## Analyze batch for PixelSNAIL prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f389b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "from src.dataloader.ffhq import FFHQWeightedDataset\n",
    "from src.dataloader.weighting import DataWeighter\n",
    "\n",
    "# Datamodule\n",
    "img_dir=\"../data/ffhq/images1024x1024\"\n",
    "img_tensor_dir=\"../data/ffhq/pt_images\"\n",
    "attr_path=\"../data/ffhq/ffhq_smile_scores.json\"\n",
    "max_property_value=5\n",
    "min_property_value=0\n",
    "mode=\"all\"\n",
    "batch_size=16\n",
    "num_workers=2 # 4\n",
    "val_split=0.1\n",
    "data_device=\"cuda\" # \"cpu\" or \"cuda\"\n",
    "\n",
    "# Weighter\n",
    "weight_type=\"uniform\"\n",
    "rank_weight_k=1e-3\n",
    "weight_quantile=None\n",
    "dbas_noise=None\n",
    "rwr_alpha=None\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    img_dir=img_dir,\n",
    "    img_tensor_dir=img_tensor_dir,\n",
    "    attr_path=attr_path,\n",
    "    max_property_value=max_property_value,\n",
    "    min_property_value=min_property_value,\n",
    "    mode=mode,\n",
    "    batch_size=2,\n",
    "    num_workers=num_workers,\n",
    "    val_split=val_split,\n",
    "    weight_type=weight_type,\n",
    "    rank_weight_k=rank_weight_k,\n",
    "    weight_quantile=weight_quantile,\n",
    "    dbas_noise=dbas_noise,\n",
    "    rwr_alpha=rwr_alpha,\n",
    "    aug=True,\n",
    "    data_device=data_device,\n",
    ")\n",
    "\n",
    "datamodule = FFHQWeightedDataset(args, DataWeighter(args))\n",
    "\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "\n",
    "latent_batch = sd_vae.encode(batch).latent_dist.sample().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "fr_nll = prior._free_run_nll(latent_batch.to(prior.device))\n",
    "print(\"Autoregressive NLL (top):\", fr_nll.item(), \"nats   \",\n",
    "      \"perplexity â‰ˆ\", math.exp(fr_nll.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
