{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779462a8",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "This notebooks determines the feature importance of the SD latent features in determining the smile score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633f418",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697bf084",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2bcd2",
   "metadata": {},
   "source": [
    "## Load latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd41354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed SD latents\n",
    "latents = torch.load(\"../data/ffhq/sd_latents.pt\", weights_only=False)\n",
    "\n",
    "# Store latent shape for later\n",
    "latent_shape = latents.shape[1:]\n",
    "\n",
    "latents = latents.reshape(latents.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d86d4a",
   "metadata": {},
   "source": [
    "### Optionally: Encode further to low-dimensional latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latent model\n",
    "from src.models.latent_models import LatentVQVAE2\n",
    "import yaml\n",
    "\n",
    "latent_model_path = \"../models/latent_vqvae2/version_1_2\"\n",
    "\n",
    "# Load latent model configuration\n",
    "latent_model_config = yaml.safe_load(\n",
    "    open(f\"{latent_model_path}/hparams.yaml\", \"r\")\n",
    ")\n",
    "\n",
    "# Initialize latent model\n",
    "latent_model = LatentVQVAE2(\n",
    "    ddconfig=latent_model_config[\"ddconfig\"],\n",
    "    lossconfig=latent_model_config[\"lossconfig\"],\n",
    "    ckpt_path=f\"{latent_model_path}/checkpoints/last.ckpt\",\n",
    ")\n",
    "latent_model = latent_model.to(device)\n",
    "latent_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d86cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Encode sd latents further into VQ latents\n",
    "batch_size = 256\n",
    "with torch.no_grad():\n",
    "    vq_latents = []\n",
    "    for i in tqdm(range(0, latents.shape[0], batch_size)):\n",
    "        batch = latents[i : i + batch_size].to(device)\n",
    "\n",
    "        # Ensure batch is 4D\n",
    "        batch = batch.view(batch.shape[0], 16, 32, 32)\n",
    "\n",
    "        # Encode the batch using the latent model\n",
    "        latents_b, latents_t, _, _ = latent_model.encode(batch)\n",
    "\n",
    "        # Flatten the two parts\n",
    "        latents_b = latents_b.view(latents_b.shape[0], -1)\n",
    "        latents_t = latents_t.view(latents_t.shape[0], -1)\n",
    "\n",
    "        # Concatenate the two parts\n",
    "        batch = torch.cat([latents_b, latents_t], dim=1)\n",
    "\n",
    "        # Move to CPU and store\n",
    "        vq_latents.append(batch.cpu())\n",
    "\n",
    "    vq_latents = torch.cat(vq_latents, dim=0)\n",
    "\n",
    "latents = vq_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f596ba5",
   "metadata": {},
   "source": [
    "## Load smile scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2626c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load smile scores\n",
    "smile_scores = json.load(open(\"../data/ffhq/ffhq_smile_scores.json\", \"r\"))\n",
    "\n",
    "# Sort by file name\n",
    "smile_scores = {k: smile_scores[k] for k in sorted(smile_scores.keys())}\n",
    "\n",
    "# Convert to tensor\n",
    "smile_scores = torch.tensor(\n",
    "    [smile_scores[k] for k in sorted(smile_scores.keys())],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "smile_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e2cdd",
   "metadata": {},
   "source": [
    "## Train Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import zero_mean_unit_var_normalization\n",
    "\n",
    "# Normalize inputs\n",
    "latents_norm, latents_mean, latents_std = zero_mean_unit_var_normalization(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f98982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Build DataLoader\n",
    "X_train = latents_norm\n",
    "y_train = smile_scores.unsqueeze(1)\n",
    "ds = TensorDataset(X_train, y_train)\n",
    "loader = DataLoader(ds, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "hidden_dims = [512, 256, 128]  # Hidden layer dimensions\n",
    "\n",
    "# Define simple MLP\n",
    "layers = []\n",
    "D = X_train.shape[1]\n",
    "prev = D\n",
    "for h in hidden_dims:\n",
    "    layers += [nn.Linear(prev, h), nn.ReLU()]\n",
    "    prev = h\n",
    "layers += [nn.Linear(prev, 1)]\n",
    "model = nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7cf48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "epochs = 1000\n",
    "lr = 1e-3\n",
    "\n",
    "# Setup progress bar to track loss\n",
    "pbar = tqdm(range(epochs), desc=\"Training Progress\", unit=\"epoch\")\n",
    "def update_pbar(epoch, loss):\n",
    "    pbar.set_postfix({\"loss\": loss.item()})\n",
    "    pbar.update(1)\n",
    "\n",
    "# Train model\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "model.train()\n",
    "for epoch in pbar:\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    update_pbar(epoch, loss)\n",
    "\n",
    "# Pickle model\n",
    "with open(\"../models/feature_selection/latents_smile_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e25c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "import pickle\n",
    "# with open(\"../models/feature_selection/sd_latents_smile_model.pkl\", \"rb\") as f:\n",
    "#     model = pickle.load(f)\n",
    "with open(\"../models/feature_selection/latents_smile_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4eb41",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d710f7",
   "metadata": {},
   "source": [
    "### Gradient-based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients w.r.t. inputs\n",
    "model.eval()\n",
    "X_all = torch.tensor(X_train, dtype=torch.float32, device=device, requires_grad=True)\n",
    "y_pred = model(X_all)\n",
    "# now backprop a uniform gradient of 1 over all outputs\n",
    "grad_outputs = torch.ones_like(y_pred)\n",
    "# Compute ∂y_pred / ∂X_all\n",
    "grads = torch.autograd.grad(\n",
    "    outputs=y_pred,\n",
    "    inputs=X_all,\n",
    "    grad_outputs=grad_outputs,\n",
    "    create_graph=False,\n",
    "    retain_graph=False,\n",
    ")[0]  # shape [N, D]\n",
    "\n",
    "# Feature importance = mean absolute gradient across samples\n",
    "gb_importances = grads.abs().mean(dim=0).cpu().numpy()  # shape (D,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57baeda",
   "metadata": {},
   "source": [
    "### Permutation-based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1989cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a subset of samples\n",
    "idx = np.random.RandomState(42).choice(len(X_train), size=5000, replace=False)\n",
    "X_sub, y_sub = X_train[idx], y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "def fast_perm_imp(model, X_train, y_train, repeats=1, batch_size=256, device=\"cuda\"):\n",
    "\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Baseline score\n",
    "    with torch.no_grad():\n",
    "        y0 = model(X_train)\n",
    "    base_mse = mse_loss(y0, y_train)\n",
    "\n",
    "    D = X_train.shape[1]\n",
    "    imps = torch.zeros(D, dtype=torch.float32, device=device)\n",
    "\n",
    "    for j in tqdm(range(D)):\n",
    "        scores = []\n",
    "        col = X_train[:, j].clone()  # copy column to restore later\n",
    "        for _ in range(repeats):\n",
    "            # shuffle column in-place\n",
    "            perm = torch.randperm(len(X_train), device=device)\n",
    "            X_train[:, j] = X_train[perm, j]\n",
    "\n",
    "            # batched predict\n",
    "            preds = []\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                xb = X_train[i : i + batch_size]\n",
    "                with torch.no_grad():\n",
    "                    preds.append(model(xb))\n",
    "            preds = torch.cat(preds)\n",
    "\n",
    "            scores.append(mse_loss(preds, y_train))\n",
    "\n",
    "            # restore column\n",
    "            X_train[:, j] = col\n",
    "\n",
    "        # importance = increase in MSE\n",
    "        scores = torch.tensor(scores, device=device)\n",
    "        imps[j] = torch.mean(scores) - base_mse\n",
    "\n",
    "    imps = imps.cpu().numpy()\n",
    "\n",
    "    return imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_importances = fast_perm_imp(model, X_sub, y_sub, repeats=5, batch_size=256, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bbfb7",
   "metadata": {},
   "source": [
    "### Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process feature importance\n",
    "gb_fi_norm = gb_importances / gb_importances.sum()\n",
    "pb_fi_norm = pb_importances / pb_importances.sum()\n",
    "\n",
    "# Sort by feature importance\n",
    "gb_sorted_indices = np.argsort(pb_fi_norm)[::-1]\n",
    "pb_sorted_indices = np.argsort(gb_fi_norm)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute cumulative feature importance\n",
    "gb_fi_cum = np.cumsum(gb_fi_norm[gb_sorted_indices])\n",
    "pb_fi_cum = np.cumsum(pb_fi_norm[pb_sorted_indices])\n",
    "\n",
    "# Get the cumulative feature importance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gb_fi_cum, label=\"Gradient-Based Importance\")\n",
    "plt.plot(pb_fi_cum, label=\"Permutation-Based Importance\")\n",
    "plt.title(\"Cumulative Feature Importance\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Cumulative Feature Importance\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f003e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pb_sorted_indices.tolist()[:512])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
