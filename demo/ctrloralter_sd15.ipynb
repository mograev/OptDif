{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8956fab9",
   "metadata": {},
   "source": [
    "# CTRLorALTer\n",
    "\n",
    "This notebook visualizes the approach of optimization in the CTRLorALTer space of a Stable Diffusion 1.5 model using a sample batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a2ef9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fe492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from src.ctrloralter.model import SD15\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Generator\n",
    "\n",
    "# Manual seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def get_generator(seed=SEED, device=\"cuda\"):\n",
    "\t\"\"\"Get a torch generator with a fixed seed.\"\"\"\n",
    "\treturn Generator(device=device).manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0fb4ab",
   "metadata": {},
   "source": [
    "### Load Evaluation Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384bc56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.load(\"../data/ffhq/eval_batch/size_512.pt\", map_location=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55675412",
   "metadata": {},
   "source": [
    "### Load Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed63cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cfg = {\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints\",\n",
    "\t\"lora\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08523d2",
   "metadata": {},
   "source": [
    "#### Style Adapter (Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.openclip import VisionModel\n",
    "from src.ctrloralter.mapper_network import SimpleMapper\n",
    "\n",
    "style_cfg_base = {\n",
    "\t\"enable\": \"always\",\n",
    "\t\"optimize\": False,\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints/sd15-style-cross-160-h\",\n",
    "\t\"ignore_check\": False,\n",
    "\t\"cfg\": True,\n",
    "\t\"transforms\": [],\n",
    "\t\"config\": {\n",
    "\t\t\"lora_scale\": 1.0,\n",
    "\t\t\"rank\": 160,\n",
    "\t\t\"c_dim\": 1024,\n",
    "\t\t\"adaption_mode\": \"only_cross\",\n",
    "\t\t\"lora_cls\": \"SimpleLoraLinear\",\n",
    "\t\t\"broadcast_tokens\": True,\n",
    "\t},\n",
    "\t\"encoder\": VisionModel(clip_model=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", local_files_only=False),\n",
    "\t\"mapper_network\": SimpleMapper(1024, 1024),\n",
    "}\n",
    "\n",
    "STYLE_FINETUNED_PATH = \"/BS/optdif/work/models/sd_lora/version_2/checkpoints/epoch_036\" # FID best after 36 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ae711",
   "metadata": {},
   "source": [
    "#### Depth Structure Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.midas import DepthEstimator\n",
    "from src.ctrloralter.mapper_network import FixedStructureMapper15\n",
    "\n",
    "depth_cfg_base = {\n",
    "\t\"enable\": \"always\",\n",
    "\t\"optimize\": False,\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints/sd15-depth-128-only-res\",\n",
    "\t\"ignore_check\": False,\n",
    "\t\"cfg\": False,\n",
    "\t\"transforms\": [],\n",
    "\t\"config\": {\n",
    "\t\t\"lora_scale\": 1.0,\n",
    "\t\t\"rank\": 128,\n",
    "\t\t\"c_dim\": 128,\n",
    "\t\t\"adaption_mode\": \"only_res_conv\",\n",
    "\t\t\"lora_cls\": \"NewStructLoRAConv\",\n",
    "\t},\n",
    "\t\"encoder\": DepthEstimator(size=512, local_files_only=False),\n",
    "\t\"mapper_network\": FixedStructureMapper15(c_dim=128),\n",
    "}\n",
    "\n",
    "STYLE_DEPTH_FINETUNED_PATH = \"/BS/optdif/work/models/sd_lora/version_3/checkpoints/epoch_000\" # FID best after one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb1b4a",
   "metadata": {},
   "source": [
    "#### HED Structure Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.hed import TorchHEDdetector\n",
    "from src.ctrloralter.mapper_network import FixedStructureMapper15\n",
    "\n",
    "hed_cfg_base = {\n",
    "\t\"enable\": \"always\",\n",
    "\t\"optimize\": False,\n",
    "\t\"ckpt_path\": \"ctrloralter/checkpoints/sd15-hed-128-only-res\",\n",
    "\t\"ignore_check\": False,\n",
    "\t\"cfg\": False,\n",
    "\t\"transforms\": [],\n",
    "\t\"config\": {\n",
    "\t\t\"lora_scale\": 1.0,\n",
    "\t\t\"rank\": 128,\n",
    "\t\t\"c_dim\": 128,\n",
    "\t\t\"adaption_mode\": \"only_res_conv\",\n",
    "\t\t\"lora_cls\": \"NewStructLoRAConv\",\n",
    "\t},\n",
    "\t\"encoder\": TorchHEDdetector(size=512, local_files_only=False),\n",
    "\t\"mapper_network\": FixedStructureMapper15(c_dim=128),\n",
    "}\n",
    "\n",
    "STYLE_HED_FINETUNED_PATH = \"/BS/optdif/work/models/sd_lora/version_6/checkpoints/epoch_000\" # loss best after five epochs, FID just gets worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee47a15",
   "metadata": {},
   "source": [
    "#### Add Adapters to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cff34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from src.ctrloralter.utils import add_lora_from_config\n",
    "\n",
    "def add_adapters(model, raw_cfg, style_cfg=None, depth_cfg=None, hed_cfg=None, device=\"cuda\"):\n",
    "\tcfg = copy.deepcopy(raw_cfg)\n",
    "\n",
    "\tif style_cfg is not None:\n",
    "\t\tcfg[\"lora\"][\"style\"] = style_cfg\n",
    "\tif depth_cfg is not None:\n",
    "\t\tcfg[\"lora\"][\"struct\"] = depth_cfg\n",
    "\telif hed_cfg is not None:\n",
    "\t\tcfg[\"lora\"][\"struct\"] = hed_cfg\n",
    "            \n",
    "\t# wrap it in a DictConfig\n",
    "\tcfg = OmegaConf.create(cfg, flags={\"allow_objects\": True})\n",
    "\n",
    "\treturn add_lora_from_config(model, cfg, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc9406",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c97a5",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ec7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c26c58",
   "metadata": {},
   "source": [
    "### Add adapters to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ad719",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ca101",
   "metadata": {},
   "source": [
    "### Predict phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = sd15.predict_phi(batch, branch_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a89fe",
   "metadata": {},
   "source": [
    "### Sample Images\n",
    "\n",
    "Sample image from the model using the obtained $\\varphi$ as condition. Note that these $\\varphi$ have not been optimized, but are the direct output of the global mapper of the style adapter. So the sampled images can't be seen as optimized images, but rather as some form reconstruction of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75117d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_images = sd15.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[\n",
    "        phi,    # style conditioning\n",
    "        batch,  # structure conditioning\n",
    "    ],\n",
    "    generator=get_generator(),\n",
    "    cfg_mask=cfg_mask, # use classifier-free guidance mask\n",
    "    skip_encode=[0], # skip encoding the first conditioning (style, already in phi)\n",
    "    skip_mapping=[0], # skip mapping the first conditioning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a106c",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input images (batch) and sampled images (sampled_images) next to each other\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, batch.shape[0], figsize=(2*batch.shape[0], 4))\n",
    "for i in range(batch.shape[0]):\n",
    "\taxes[0, i].imshow(batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "\taxes[0, i].axis('off')\n",
    "\taxes[1, i].imshow(sampled_images[i])\n",
    "\taxes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f04bbd",
   "metadata": {},
   "source": [
    "## Supplements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec0079",
   "metadata": {},
   "source": [
    "### Visualize Depth Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e56461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.midas import DepthEstimator\n",
    "\n",
    "de = DepthEstimator(size=512, local_files_only=False).to(\"cuda\").eval()\n",
    "\n",
    "depths = de(batch)\n",
    "depths = depths.mean(dim=1, keepdim=True)  # Average over the color\n",
    "\n",
    "# Visualize input images (batch) and their corresponding depth maps\n",
    "fig, axes = plt.subplots(2, batch.shape[0], figsize=(2*batch.shape[0], 4))\n",
    "for i in range(batch.shape[0]):\n",
    "\taxes[0, i].imshow(batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "\taxes[0, i].axis('off')\n",
    "\taxes[1, i].imshow(depths[i].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "\taxes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa16d5",
   "metadata": {},
   "source": [
    "### Visualize HED Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1674df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ctrloralter.annotators.hed import TorchHEDdetector\n",
    "\n",
    "hed = TorchHEDdetector(size=512, local_files_only=False).to(\"cuda\").eval()\n",
    "\n",
    "edges = hed(batch)\n",
    "edges = edges.mean(dim=1, keepdim=True)  # Average over the color\n",
    "\n",
    "# Visualize input images (batch) and their corresponding depth maps\n",
    "fig, axes = plt.subplots(2, batch.shape[0], figsize=(2*batch.shape[0], 4))\n",
    "for i in range(batch.shape[0]):\n",
    "\taxes[0, i].imshow(batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "\taxes[0, i].axis('off')\n",
    "\taxes[1, i].imshow(edges[i].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "\taxes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062f9a3",
   "metadata": {},
   "source": [
    "## Comparison Conditioning Opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2cff4",
   "metadata": {},
   "source": [
    "#### Direct style reconstructions (without structure adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "# Add only style adapter\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg_base)\n",
    "\n",
    "# Predict phi\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "# Sample style images\n",
    "style = sd15.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi],\n",
    "    generator=get_generator(),\n",
    "    cfg_mask=cfg_mask,\n",
    "    skip_encode=True,\n",
    "    skip_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09c94c",
   "metadata": {},
   "source": [
    "#### Style + Depth Structure Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa691da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "depth_model = DepthEstimator(size=512, local_files_only=False).to(\"cuda\").eval()\n",
    "depth_maps = depth_model(batch)\n",
    "depth_maps = depth_maps.mean(dim=1, keepdim=True)  # average over color channels\n",
    "\n",
    "# Add only style adapter\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg_base, depth_cfg=depth_cfg_base)\n",
    "\n",
    "# Predict phi\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "# Sample images\n",
    "style_depth = sd15.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi, batch],  # style and structure conditioning\n",
    "    generator=get_generator(),\n",
    "    cfg_mask=cfg_mask,\n",
    "    skip_encode=[0],\n",
    "    skip_mapping=[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd76358",
   "metadata": {},
   "source": [
    "#### Style + HED Structure Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "hed_model = TorchHEDdetector(size=512, local_files_only=False).to(\"cuda\").eval()\n",
    "hed_maps = hed_model(batch)\n",
    "hed_maps = hed_maps.mean(dim=1, keepdim=True)  # average over color channels\n",
    "\n",
    "# Add only style adapter\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg_base, hed_cfg=hed_cfg_base)\n",
    "\n",
    "# Predict phi\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "# Sample images\n",
    "style_hed = sd15.sample_custom(\n",
    "    prompt=\"\",\n",
    "    num_images_per_prompt=batch.shape[0],\n",
    "    cs=[phi, batch],  # style and structure conditioning\n",
    "    generator=get_generator(),\n",
    "    cfg_mask=cfg_mask,\n",
    "    skip_encode=[0],\n",
    "    skip_mapping=[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e156585e",
   "metadata": {},
   "source": [
    "#### Visualize all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0bc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(6, batch.shape[0], figsize=(2*batch.shape[0], 2.2*6), squeeze=False)\n",
    "\n",
    "# Row 0: Original images\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, i].imshow(img)\n",
    "\tax[0, i].axis('off')\n",
    "\n",
    "# Row 1: Style images\n",
    "ax[1, 0].set_title(\"Reconstruction based on Style\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[1, i].imshow(style[i])\n",
    "\tax[1, i].axis('off')\n",
    "\n",
    "# Row 2: Depth Maps\n",
    "ax[2, 0].set_title(\"Depth Maps\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (depth_maps[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[2, i].imshow(img, cmap='gray')\n",
    "\tax[2, i].axis('off')\n",
    "\n",
    "# Row 3: Style + Depth\n",
    "ax[3, 0].set_title(\"Reconstruction based on Style + Depth\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[3, i].imshow(style_depth[i])\n",
    "\tax[3, i].axis('off')\n",
    "\n",
    "# Row 4: HED Maps\n",
    "ax[4, 0].set_title(\"HED Maps\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (hed_maps[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[4, i].imshow(img, cmap='gray')\n",
    "\tax[4, i].axis('off')\n",
    "\n",
    "# Row 5: Style + HED\n",
    "ax[5, 0].set_title(\"Reconstruction based on Style + HED\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[5, i].imshow(style_hed[i])\n",
    "\tax[5, i].axis('off')\n",
    "\t\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92fbeb",
   "metadata": {},
   "source": [
    "## LoRA Scale Ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d3920",
   "metadata": {},
   "source": [
    "### Direct style reconstructions (without structure adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "style_scales = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6]\n",
    "\n",
    "for scale in style_scales:\n",
    "\tprint(f\"Testing lora scale: {scale}\")\n",
    "\n",
    "\t# Load model\n",
    "\tsd15 = SD15(\n",
    "\t\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\t\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\t\tlocal_files_only=False,\n",
    "\t).cuda().eval()\n",
    "\n",
    "\t# Set LoRA scale\n",
    "\tstyle_cfg = copy.deepcopy(style_cfg_base)\n",
    "\tstyle_cfg[\"config\"][\"lora_scale\"] = scale\n",
    "\tcfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg)\n",
    "\n",
    "\t# Predict phi\n",
    "\tphi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "\t# Sample style images\n",
    "\tstyle = sd15.sample_custom(\n",
    "\t\tprompt=\"\",\n",
    "\t\tnum_images_per_prompt=batch.shape[0],\n",
    "\t\tcs=[phi],\n",
    "\t\tgenerator=get_generator(),\n",
    "\t\tcfg_mask=cfg_mask,\n",
    "\t\tskip_encode=True,\n",
    "\t\tskip_mapping=True,\n",
    "\t)\n",
    "\n",
    "\t# Append to results\n",
    "\tresults[scale] = style\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(len(results)+1, batch.shape[0], figsize=(2.2*batch.shape[0], 2.2*(len(results)+1)), squeeze=False)\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for j in range(batch.shape[0]):\n",
    "\timg = (batch[j].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, j].imshow(img)\n",
    "\tax[0, j].axis('off')\n",
    "for i, (scale, images) in enumerate(results.items()):\n",
    "\tax[i+1, 0].set_title(f\"LoRA Scale: {scale:.1f}\", loc=\"left\")\n",
    "\tfor j, img in enumerate(images):\n",
    "\t\tax[i+1, j].imshow(img)\n",
    "\t\tax[i+1, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd358bb",
   "metadata": {},
   "source": [
    "### Style + Depth Structure Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If two scales are ablated, visualize effects only on one image\n",
    "eval_sample = batch[6].clone()\n",
    "\n",
    "style_scales = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6]\n",
    "depth_scales = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6]\n",
    "# style_scales = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "# depth_scales = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "results = {}\n",
    "for style_scale in style_scales:\n",
    "\tfor depth_scale in depth_scales:\n",
    "\t\tprint(f\"Testing lora scale: {style_scale}, {depth_scale}\")\n",
    "\n",
    "\t\t# Load model\n",
    "\t\tsd15 = SD15(\n",
    "\t\t\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\t\t\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\t\t\tlocal_files_only=False,\n",
    "\t\t).cuda().eval()\n",
    "\n",
    "\t\t# Set LoRA scale\n",
    "\t\tstyle_cfg = copy.deepcopy(style_cfg_base)\n",
    "\t\tdepth_cfg = copy.deepcopy(depth_cfg_base)\n",
    "\t\tstyle_cfg[\"config\"][\"lora_scale\"] = style_scale\n",
    "\t\tdepth_cfg[\"config\"][\"lora_scale\"] = depth_scale\n",
    "\t\tcfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg, depth_cfg=depth_cfg)\n",
    "\n",
    "\t\t# Predict phi\n",
    "\t\tphi = sd15.predict_phi(eval_sample.unsqueeze(0), branch_idx=0)\n",
    "\t\tprint(f\"Phi shape: {phi.shape}\")\n",
    "\n",
    "\t\t# Sample style images\n",
    "\t\tstyle_depth = sd15.sample_custom(\n",
    "\t\t\tprompt=\"\",\n",
    "\t\t\tnum_images_per_prompt=1,\n",
    "\t\t\tcs=[phi, eval_sample.unsqueeze(0)],  # style and structure conditioning\n",
    "\t\t\tgenerator=get_generator(),\n",
    "\t\t\tcfg_mask=cfg_mask,\n",
    "\t\t\tskip_encode=[0],\n",
    "\t\t\tskip_mapping=[0],\n",
    "\t\t)\n",
    "\n",
    "\t\t# Append to results\n",
    "\t\tresults[(style_scale, depth_scale)] = style_depth\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(len(style_scales)+1, len(depth_scales)+1, figsize=(2.2*(len(depth_scales)+1), 2.2*(len(style_scales)+1)))\n",
    "# First row and column is the original image\n",
    "ax[0, 0].set_title(\"Original Image\")\n",
    "ax[0, 0].imshow((eval_sample.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1))\n",
    "ax[0, 0].axis('off')\n",
    "# Label rows and columns based on style and depth scales\n",
    "for i, s in enumerate(style_scales):\n",
    "    ax[i+1, 0].text(0.5, 0.5, f\"Style Scale:\\n{s:.1f}\", fontsize=14, va=\"center\", ha=\"center\")\n",
    "    ax[i+1, 0].axis('off')\n",
    "for j, d in enumerate(depth_scales):\n",
    "    ax[0, j+1].text(0.5, 0.5, f\"Depth Scale:\\n{d:.1f}\", fontsize=14, va=\"center\", ha=\"center\")\n",
    "    ax[0, j+1].axis('off')\n",
    "# Grid of images\n",
    "for i, style_scale in enumerate(style_scales):\n",
    "\tfor j, depth_scale in enumerate(depth_scales):\n",
    "\t\tax[i+1, j+1].imshow(results[(style_scale, depth_scale)][0])\n",
    "\t\tax[i+1, j+1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15f557",
   "metadata": {},
   "source": [
    "### Style + HED Structure Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If two scales are ablated, visualize effects only on one image\n",
    "eval_sample = batch[6].clone()\n",
    "\n",
    "style_scales = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6]\n",
    "hed_scales = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6]\n",
    "\n",
    "results = {}\n",
    "for style_scale in style_scales:\n",
    "\tfor hed_scale in hed_scales:\n",
    "\t\tprint(f\"Testing lora scale: {style_scale}, {hed_scale}\")\n",
    "\n",
    "\t\t# Load model\n",
    "\t\tsd15 = SD15(\n",
    "\t\t\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\t\t\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\t\t\tlocal_files_only=False,\n",
    "\t\t).cuda().eval()\n",
    "\n",
    "\t\t# Set LoRA scale\n",
    "\t\tstyle_cfg = copy.deepcopy(style_cfg_base)\n",
    "\t\thed_cfg = copy.deepcopy(hed_cfg_base)\n",
    "\t\tstyle_cfg[\"config\"][\"lora_scale\"] = style_scale\n",
    "\t\thed_cfg[\"config\"][\"lora_scale\"] = hed_scale\n",
    "\t\tcfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg, hed_cfg=hed_cfg)\n",
    "\n",
    "\t\t# Predict phi\n",
    "\t\tphi = sd15.predict_phi(eval_sample.unsqueeze(0), branch_idx=0)\n",
    "\t\tprint(f\"Phi shape: {phi.shape}\")\n",
    "\n",
    "\t\t# Sample style images\n",
    "\t\tstyle_hed = sd15.sample_custom(\n",
    "\t\t\tprompt=\"\",\n",
    "\t\t\tnum_images_per_prompt=1,\n",
    "\t\t\tcs=[phi, eval_sample.unsqueeze(0)],  # style and structure conditioning\n",
    "\t\t\tgenerator=get_generator(),\n",
    "\t\t\tcfg_mask=cfg_mask,\n",
    "\t\t\tskip_encode=[0],\n",
    "\t\t\tskip_mapping=[0],\n",
    "\t\t)\n",
    "\n",
    "\t\t# Append to results\n",
    "\t\tresults[(style_scale, hed_scale)] = style_hed\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(len(style_scales)+1, len(hed_scales)+1, figsize=(2.2*(len(hed_scales)+1), 2.2*(len(style_scales)+1)))\n",
    "# First row and column is the original image\n",
    "ax[0, 0].set_title(\"Original Image\")\n",
    "ax[0, 0].imshow((eval_sample.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1))\n",
    "ax[0, 0].axis('off')\n",
    "# Label rows and columns based on style and depth scales\n",
    "for i, s in enumerate(style_scales):\n",
    "    ax[i+1, 0].text(0.5, 0.5, f\"Style Scale:\\n{s:.1f}\", fontsize=14, va=\"center\", ha=\"center\")\n",
    "    ax[i+1, 0].axis('off')\n",
    "for j, d in enumerate(hed_scales):\n",
    "    ax[0, j+1].text(0.5, 0.5, f\"HED Scale:\\n{d:.1f}\", fontsize=14, va=\"center\", ha=\"center\")\n",
    "    ax[0, j+1].axis('off')\n",
    "# Grid of images\n",
    "for i, style_scale in enumerate(style_scales):\n",
    "\tfor j, hed_scale in enumerate(hed_scales):\n",
    "\t\tax[i+1, j+1].imshow(results[(style_scale, hed_scale)][0])\n",
    "\t\tax[i+1, j+1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc71af",
   "metadata": {},
   "source": [
    "## LoRA Fine-Tuning Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e073158",
   "metadata": {},
   "source": [
    "### Style Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe07ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Initial style\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg_base)\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "initial_style = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi],  # style and structure conditioning\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=True,\n",
    "\tskip_mapping=True,\n",
    ")\n",
    "\n",
    "# 2) Finetuned style\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "# Set checkpoint path to fine-tuned model\n",
    "style_cfg = copy.deepcopy(style_cfg_base)\n",
    "style_cfg[\"ckpt_path\"] = STYLE_FINETUNED_PATH\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg)\n",
    "\n",
    "# Predict phi\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "# Sample images\n",
    "finetuned_style = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi],  # style and structure conditioning\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=True,\n",
    "\tskip_mapping=True,\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(3, batch.shape[0], figsize=(2*batch.shape[0], 2.2*3), squeeze=False)\n",
    "\n",
    "# Row 0: Original images\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, i].imshow(img)\n",
    "\tax[0, i].axis('off')\n",
    "\n",
    "# Row 1: Initial Style\n",
    "ax[1, 0].set_title(\"Reconstruction based on Initial Style\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[1, i].imshow(initial_style[i])\n",
    "\tax[1, i].axis('off')\n",
    "\n",
    "# Row 2: Finetuned Style\n",
    "ax[2, 0].set_title(\"Reconstruction based on Finetuned Style\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[2, i].imshow(finetuned_style[i])\n",
    "\tax[2, i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e26e78",
   "metadata": {},
   "source": [
    "### Style + Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Initial Style + Initial Depth\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg_base, depth_cfg=depth_cfg_base)\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "initial_style_initial_depth = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi, batch],\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=[0],\n",
    "\tskip_mapping=[0],\n",
    ")\n",
    "\n",
    "# 2) Initial Style + Finetuned Depth\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "depth_cfg = copy.deepcopy(depth_cfg_base)\n",
    "depth_cfg[\"ckpt_path\"] = STYLE_DEPTH_FINETUNED_PATH\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg_base, depth_cfg=depth_cfg)\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "initial_style_finetuned_depth = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi, batch],\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=[0],\n",
    "\tskip_mapping=[0],\n",
    ")\n",
    "\n",
    "# 3) Finetuned Style + Initial Depth\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "style_cfg = copy.deepcopy(style_cfg_base)\n",
    "style_cfg[\"ckpt_path\"] = STYLE_FINETUNED_PATH\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg, depth_cfg=depth_cfg_base)\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "finetuned_style_initial_depth = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi, batch],\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=[0],\n",
    "\tskip_mapping=[0],\n",
    ")\n",
    "\n",
    "# 4) Finetuned Style + Finetuned Depth\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "style_cfg = copy.deepcopy(style_cfg_base)\n",
    "depth_cfg = copy.deepcopy(depth_cfg_base)\n",
    "style_cfg[\"ckpt_path\"] = STYLE_DEPTH_FINETUNED_PATH\n",
    "depth_cfg[\"ckpt_path\"] = STYLE_DEPTH_FINETUNED_PATH\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg, depth_cfg=depth_cfg)\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "finetuned_style_finetuned_depth = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi, batch],\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=[0],\n",
    "\tskip_mapping=[0],\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(5, batch.shape[0], figsize=(2*batch.shape[0], 2.2*5), squeeze=False)\n",
    "\n",
    "# Row 0: Original images\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, i].imshow(img)\n",
    "\tax[0, i].axis('off')\n",
    "\n",
    "# Row 1: Initial Style + Initial Depth\n",
    "ax[1, 0].set_title(\"Reconstruction based on Initial Style + Initial Depth\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[1, i].imshow(initial_style_initial_depth[i])\n",
    "\tax[1, i].axis('off')\n",
    "\n",
    "# Row 2: Initial Style + Finetuned Depth\n",
    "ax[2, 0].set_title(\"Reconstruction based on Initial Style + Finetuned Depth\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[2, i].imshow(initial_style_finetuned_depth[i])\n",
    "\tax[2, i].axis('off')\n",
    "\n",
    "# Row 3: Finetuned Style + Initial Depth\n",
    "ax[3, 0].set_title(\"Reconstruction based on Finetuned Style + Initial Depth\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[3, i].imshow(finetuned_style_initial_depth[i])\n",
    "\tax[3, i].axis('off')\n",
    "\n",
    "# Row 4: Finetuned Style + Finetuned Depth\n",
    "ax[4, 0].set_title(\"Reconstruction based on Finetuned Style + Finetuned Depth\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[4, i].imshow(finetuned_style_finetuned_depth[i])\n",
    "\tax[4, i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5111c05",
   "metadata": {},
   "source": [
    "### Style + HED Structure Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Initial Style + Initial HED\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg_base, hed_cfg=hed_cfg_base)\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "initial_style_initial_hed = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi, batch],\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=[0],\n",
    "\tskip_mapping=[0],\n",
    ")\n",
    "\n",
    "# 2) Initial Style + Finetuned HED\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "hed_cfg = copy.deepcopy(hed_cfg_base)\n",
    "hed_cfg[\"ckpt_path\"] = STYLE_HED_FINETUNED_PATH\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg_base, hed_cfg=hed_cfg)\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "initial_style_finetuned_hed = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi, batch],\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=[0],\n",
    "\tskip_mapping=[0],\n",
    ")\n",
    "\n",
    "# 3) Finetuned Style + Initial HED\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "style_cfg = copy.deepcopy(style_cfg_base)\n",
    "style_cfg[\"ckpt_path\"] = STYLE_FINETUNED_PATH\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg, hed_cfg=hed_cfg_base)\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "finetuned_style_initial_hed = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi, batch],\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=[0],\n",
    "\tskip_mapping=[0],\n",
    ")\n",
    "\n",
    "# 4) Finetuned Style + Finetuned HED\n",
    "sd15 = SD15(\n",
    "\tpipeline_type=\"diffusers.StableDiffusionPipeline\",\n",
    "\tmodel_name=\"runwayml/stable-diffusion-v1-5\",\n",
    "\tlocal_files_only=False,\n",
    ").cuda().eval()\n",
    "\n",
    "style_cfg = copy.deepcopy(style_cfg_base)\n",
    "hed_cfg = copy.deepcopy(hed_cfg_base)\n",
    "style_cfg[\"ckpt_path\"] = STYLE_HED_FINETUNED_PATH\n",
    "hed_cfg[\"ckpt_path\"] = STYLE_HED_FINETUNED_PATH\n",
    "cfg_mask = add_adapters(sd15, raw_cfg, style_cfg=style_cfg, hed_cfg=hed_cfg)\n",
    "phi = sd15.predict_phi(batch, branch_idx=0)\n",
    "\n",
    "finetuned_style_finetuned_hed = sd15.sample_custom(\n",
    "\tprompt=\"\",\n",
    "\tnum_images_per_prompt=batch.shape[0],\n",
    "\tcs=[phi, batch],\n",
    "\tgenerator=get_generator(),\n",
    "\tcfg_mask=cfg_mask,\n",
    "\tskip_encode=[0],\n",
    "\tskip_mapping=[0],\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(5, batch.shape[0], figsize=(2*batch.shape[0], 2.2*5), squeeze=False)\n",
    "\n",
    "# Row 0: Original images\n",
    "ax[0, 0].set_title(\"Original Images\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\timg = (batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1)\n",
    "\tax[0, i].imshow(img)\n",
    "\tax[0, i].axis('off')\n",
    "\n",
    "# Row 1: Initial Style + Initial HED\n",
    "ax[1, 0].set_title(\"Reconstruction based on Initial Style + Initial HED\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[1, i].imshow(initial_style_initial_hed[i])\n",
    "\tax[1, i].axis('off')\n",
    "\n",
    "# Row 2: Initial Style + Finetuned HED\n",
    "ax[2, 0].set_title(\"Reconstruction based on Initial Style + Finetuned HED\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[2, i].imshow(initial_style_finetuned_hed[i])\n",
    "\tax[2, i].axis('off')\n",
    "\n",
    "# Row 3: Finetuned Style + Initial HED\n",
    "ax[3, 0].set_title(\"Reconstruction based on Finetuned Style + Initial HED\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[3, i].imshow(finetuned_style_initial_hed[i])\n",
    "\tax[3, i].axis('off')\n",
    "\n",
    "# Row 4: Finetuned Style + Finetuned HED\n",
    "ax[4, 0].set_title(\"Reconstruction based on Finetuned Style + Finetuned HED\", loc=\"left\")\n",
    "for i in range(batch.shape[0]):\n",
    "\tax[4, i].imshow(finetuned_style_finetuned_hed[i])\n",
    "\tax[4, i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
