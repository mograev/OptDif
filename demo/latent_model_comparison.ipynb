{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Model Comparison\n",
    "\n",
    "This notebook compares different latent models and hyperparameter settings, and the corresponding training results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.latent_models import *\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Configuration\n",
    "MODEL_LIST = [\n",
    "    # MODEL_TYPE, VERSION\n",
    "    (\"LatentVAE\", 16),\n",
    "\t# (\"LatentVAE\", 20),\n",
    "    (\"LatentVAE\", 19),\n",
    "    # (\"LatentVAE\", 18),\n",
    "    # (\"LatentVQVAE\", 8),\n",
    "    # (\"LatentVQVAE2\", 0),\n",
    "    # (\"LatentVQVAE2\", \"1_2\"),\n",
    "    # (\"LatentVQVAE\", 8),\n",
    "    # (\"LatentVQVAE2\", 0),\n",
    "    # (\"LatentVQVAE2\", \"1_2\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Configuration\n",
    "model_type_mapping = {\n",
    "    \"LatentVAE\": \"latent_vae\",\n",
    "    \"LatentVQVAE\": \"latent_vqvae\",\n",
    "    \"LatentVQVAE2\": \"latent_vqvae2\",\n",
    "    \"LatentAutoencoder\": \"latent_autoencoder\",\n",
    "    \"LatentLinearAE\": \"latent_linear_ae\"\n",
    "}\n",
    "model_type_sub = [model_type_mapping[MODEL_LIST[i][0]] for i in range(len(MODEL_LIST))]\n",
    "config_path = [f\"../models/{model_type_sub[i]}/version_{MODEL_LIST[i][1]}/hparams.yaml\" for i in range(len(MODEL_LIST))]\n",
    "ckpt_path = [f\"../models/{model_type_sub[i]}/version_{MODEL_LIST[i][1]}/checkpoints/last.ckpt\" for i in range(len(MODEL_LIST))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Latent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model config\n",
    "config = []\n",
    "for i in range(len(MODEL_LIST)):\n",
    "    with open(config_path[i], \"r\") as f:\n",
    "        config.append(yaml.safe_load(f))\n",
    "\n",
    "# Extract model class\n",
    "model_cls_mapping = {\n",
    "    \"LatentVAE\": LatentVAE,\n",
    "    \"LatentVQVAE\": LatentVQVAE,\n",
    "    \"LatentVQVAE2\": LatentVQVAE2,\n",
    "    \"LatentAutoencoder\": LatentAutoencoder,\n",
    "    \"LatentLinearAE\": LatentLinearAE\n",
    "}\n",
    "model_cls = [model_cls_mapping[MODEL_LIST[i][0]] for i in range(len(MODEL_LIST))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "model = []\n",
    "for i in range(len(MODEL_LIST)):\n",
    "    curr_model = model_cls[i](\n",
    "        ddconfig=config[i][\"ddconfig\"],\n",
    "        lossconfig=config[i][\"lossconfig\"],\n",
    "        ckpt_path=ckpt_path[i],\n",
    "        ignore_keys=['loss'],\n",
    "    )\n",
    "    curr_model.eval()\n",
    "    model.append(curr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stable Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion VAE model\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "sd_vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-3.5-medium\", subfolder=\"vae\")\n",
    "sd_vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Decoding Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "NUM_SAMPLES = 4\n",
    "\n",
    "# Load (subset of) eval batch\n",
    "eval_batch = torch.load(\"../data/ffhq/eval/batch_256.pt\")\n",
    "eval_batch = eval_batch[:NUM_SAMPLES]\n",
    "\n",
    "# Initialize lists to store images and reconstructions\n",
    "input_img = []\n",
    "recon_img = [\n",
    "    [] for _ in range(len(MODEL_LIST))\n",
    "]\n",
    "\n",
    "# Loop through the selected images\n",
    "for i in range(NUM_SAMPLES):\n",
    "    # Add batch dimension: Change from (C, H, W) to (B, C, H, W)\n",
    "    img_tensor = eval_batch[i].unsqueeze(0)\n",
    "    # Encode the image using the Stable Diffusion VAE\n",
    "    sd_latent = sd_vae.encode(img_tensor).latent_dist.sample()\n",
    "    for i, model_i in enumerate(model):\n",
    "        # Encode and decode the latent using the model\n",
    "        recon = model_i(sd_latent, return_only_recon=True)\n",
    "        # Decode the latents using the Stable Diffusion VAE\n",
    "        sd_recon = sd_vae.decode(recon).sample\n",
    "        # Clip the reconstructions to the range [-1, 1]\n",
    "        sd_recon = torch.clamp(sd_recon, -1, 1)\n",
    "        # Store the images\n",
    "        recon_img[i].append(sd_recon.squeeze(0).permute(1, 2, 0).detach().cpu().numpy())\n",
    "    input_img.append(img_tensor.squeeze(0).permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = len(MODEL_LIST) + 1\n",
    "num_rows = NUM_SAMPLES\n",
    "\n",
    "# Plot the input and reconstructed images\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(num_cols*5, num_rows*5))\n",
    "for i in range(num_rows):\n",
    "    # Plot the input image\n",
    "    ax[0, 0].set_title(\"Input Image\", fontsize=25)\n",
    "    ax[i, 0].imshow((input_img[i] + 1) / 2)\n",
    "    ax[i, 0].axis('off')\n",
    "\n",
    "    # Plot the reconstructed images\n",
    "    for j in range(len(MODEL_LIST)):\n",
    "        ax[0, j + 1].set_title(f\"{MODEL_LIST[j][0]} (v{MODEL_LIST[j][1]})\", fontsize=25)\n",
    "        ax[i, j + 1].imshow((recon_img[j][i] + 1) / 2)\n",
    "        ax[i, j + 1].axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CUSTOM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = len(MODEL_LIST) + 1\n",
    "num_rows = NUM_SAMPLES\n",
    "\n",
    "# Plot the input and reconstructed images\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(num_cols*5, num_rows*5))\n",
    "for i in range(num_rows):\n",
    "    # Plot the input image\n",
    "    ax[i, 0].imshow((input_img[i] + 1) / 2)\n",
    "    ax[i, 0].axis('off')\n",
    "\n",
    "    # Plot the reconstructed images\n",
    "    for j in range(len(MODEL_LIST)):\n",
    "        ax[i, j + 1].imshow((recon_img[j][i] + 1) / 2)\n",
    "        ax[i, j + 1].axis('off')\n",
    "        \n",
    "\t# Set custom titles\n",
    "    ax[0, 0].set_title(\"Input Image\", fontsize=25)\n",
    "    # ax[0, 1].set_title(f\"LatentVQVAE\", fontsize=25)\n",
    "    # ax[0, 2].set_title(f\"LatentVQVAE2\", fontsize=25)\n",
    "    # ax[0, 3].set_title(f\"LatentVQVAE2 (2x size)\", fontsize=25)\n",
    "    ax[0, 1].set_title(f\"16k → 512\", fontsize=25)\n",
    "    ax[0, 2].set_title(f\"16k → 4k\", fontsize=25)\n",
    "    # ax[0, 3].set_title(f\"16k → 4k\", fontsize=25)\n",
    "    # ax[0, 4].set_title(f\"16k → 8k\", fontsize=25)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
