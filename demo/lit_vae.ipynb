{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook show how to load a pretrained model from the diffusers library and encapsulate it in a module such that it can be used in a pytorch lightning Trainer pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "from src.models.lit_vae import LitVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the VAE model from the Huggingface hub\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-3.5-medium\", subfolder=\"vae\")\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch lightning module from the VAE model\n",
    "lit_vae = LitVAE(vae)\n",
    "lit_vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "from src.dataloader.ffhq import FFHQWeightedTensorDataset\n",
    "from src.dataloader.weighting import DataWeighter\n",
    "\n",
    "# Datamodule\n",
    "img_dir=\"/pfs/work7/workspace/scratch/ma_mgraevin-optdif/data/ffhq/images1024x1024\"\n",
    "pt_dir=\"/pfs/work7/workspace/scratch/ma_mgraevin-optdif/data/ffhq/pt_images\"\n",
    "train_attr_path=\"/pfs/work7/workspace/scratch/ma_mgraevin-optdif/data/ffhq/ffhq_smile_scores.json\"\n",
    "val_attr_path=\"/pfs/work7/workspace/scratch/ma_mgraevin-optdif/data/ffhq/ffhq_smile_scores.json\"\n",
    "combined_attr_path=\"/pfs/work7/workspace/scratch/ma_mgraevin-optdif/data/ffhq/ffhq_smile_scores.json\"\n",
    "max_property_value=5\n",
    "min_property_value=0\n",
    "mode=\"all\"\n",
    "batch_size=128\n",
    "num_workers=2 # 4\n",
    "\n",
    "# Weighter\n",
    "weight_type=\"uniform\"\n",
    "rank_weight_k=1e-3\n",
    "weight_quantile=None\n",
    "dbas_noise=None\n",
    "rwr_alpha=None\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    img_dir=img_dir,\n",
    "    pt_dir=pt_dir,\n",
    "    train_attr_path=train_attr_path,\n",
    "    val_attr_path=val_attr_path,\n",
    "    combined_attr_path=combined_attr_path,\n",
    "    max_property_value=max_property_value,\n",
    "    min_property_value=min_property_value,\n",
    "    mode=mode,\n",
    "    batch_size=2,\n",
    "    num_workers=num_workers,\n",
    "    weight_type=weight_type,\n",
    "    rank_weight_k=rank_weight_k,\n",
    "    weight_quantile=weight_quantile,\n",
    "    dbas_noise=dbas_noise,\n",
    "    rwr_alpha=rwr_alpha\n",
    ")\n",
    "\n",
    "datamodule = FFHQWeightedTensorDataset(args, DataWeighter(args))\n",
    "datamodule.setup() # assignment into train/validation split is made and weights are set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one batch of data\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply forward pass of the VAE model\n",
    "recon_batch, latent_dist = lit_vae(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist.var"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
