{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Export\n",
    "\n",
    "This notebook exports data from Tensorboard to a csv file for several metrics/losses simultaneously. The code is based on a [StackOverflow post](https://stackoverflow.com/questions/72837772/download-all-of-csv-files-of-tensorboard-at-once).\n",
    "\n",
    "In order for this code to work, you need to have Tensorboard installed and running, and you need to specify the log directory where your Tensorboard logs are stored. For each of the model families, the tensorboard can be launched with the following command:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir models/{MODEL}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tb_data(version, split):\n",
    "\n",
    "    # Get epoch data\n",
    "    url = f'http://localhost:6006/experiment/defaultExperimentId/data/plugin/scalars/scalars?tag=epoch&run=version_{version}&format=csv'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    data_csv = list(reader(r.text.splitlines()))\n",
    "    result_df = pd.DataFrame(data_csv[1:], columns=data_csv[0])\n",
    "    result_df.drop(columns=['Wall time'], inplace=True)\n",
    "    result_df.rename(columns={'Step': 'step', 'Value': 'epoch'}, inplace=True)\n",
    "    result_df['epoch'] = result_df['epoch'].astype(float)\n",
    "    result_df['step'] = result_df['step'].astype(float)\n",
    "    # Fill in missing steps (carry the last known epoch)\n",
    "    full_steps = np.arange(1, result_df[\"step\"].max() + 1)\n",
    "    result_df = (\n",
    "        pd.merge_asof(\n",
    "            pd.DataFrame({\"step\": full_steps}),\n",
    "            result_df.sort_values(\"step\"),\n",
    "            on=\"step\",\n",
    "            direction=\"forward\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get other data\n",
    "    dfs = {}\n",
    "    possible_metrics = [\n",
    "        'disc_loss', 'disc_active', 'gen_loss', 'g_loss', # GAN losses\n",
    "        'kl_loss', 'nll_loss', # VAE losses\n",
    "        'perc_img_loss', # Perceptual loss\n",
    "        'rec_loss', 'rec_img_loss', 'rec_lat_loss', # Reconstruction losses\n",
    "        'codebook_loss', 'perplexity', # VQ-VAE losses\n",
    "        'loss_top','loss_bottom', # VQ-VAE2 losses\n",
    "        'fid_score', 'spectral_score', # Evaluation metrics\n",
    "        'total_loss', 'loss' # General losses\n",
    "    ]\n",
    "    possible_intervals = ['', '_step', '_epoch']\n",
    "\n",
    "    for metric in possible_metrics:\n",
    "        for interval in possible_intervals:\n",
    "            key = f'{split}/{metric}{interval}'\n",
    "            url = f'http://localhost:6006/experiment/defaultExperimentId/data/plugin/scalars/scalars?tag={split}%2F{metric}{interval}&run=version_{version}&format=csv'\n",
    "\n",
    "            try:\n",
    "                r = requests.get(url, allow_redirects=True)\n",
    "                data_csv = list(reader(r.text.splitlines()))\n",
    "                df = pd.DataFrame(data_csv[1:], columns=data_csv[0])\n",
    "                if not df.empty:\n",
    "                    df.drop(columns=['Wall time'], inplace=True)\n",
    "                    df.rename(columns={'Step': 'step', 'Value': key}, inplace=True)\n",
    "                    df['step'] = df['step'].astype(float)\n",
    "                    dfs[key] = df\n",
    "                    break\n",
    "            except:\n",
    "                print(f\"{key} not found.\")\n",
    "                continue\n",
    "\n",
    "    # Merge dataframes\n",
    "    for key, df in dfs.items():\n",
    "        result_df = pd.merge(result_df, df, on='step', how='right')\n",
    "\n",
    "    # Sort by 'Step' (int)\n",
    "    result_df.sort_values(by='step', inplace=True)\n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'latent_vae'\n",
    "VERSION = 16\n",
    "\n",
    "SPLIT = 'val'\n",
    "df = tb_data(VERSION, SPLIT)\n",
    "\n",
    "# export df to csv\n",
    "df.to_csv(f'losses/{MODEL}_v{VERSION}_{SPLIT}_losses.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optdif1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
